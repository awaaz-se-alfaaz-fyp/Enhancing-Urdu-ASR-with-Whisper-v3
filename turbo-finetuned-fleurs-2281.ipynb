{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f84b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A40\n",
      "47.69972224 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5207d25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaider/miniconda3/envs/whisper-lora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/shaider/miniconda3/envs/whisper-lora/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.57.3\n",
      "Python version: 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]\n",
      "Transformers file: /home/shaider/miniconda3/envs/whisper-lora/lib/python3.10/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers, sys, importlib.util\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"Transformers file:\", importlib.util.find_spec(\"transformers\").origin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597c63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# IMPORTS\n",
    "# ================================\n",
    "\n",
    "import os, csv, tarfile, glob, time, datetime, random, torch, re, evaluate, unicodedata, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import papermill as pm\n",
    "\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset, Dataset, Audio, DatasetDict, concatenate_datasets, Features, Value\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "from jiwer import wer as jiwer_wer\n",
    "from huggingface_hub import login\n",
    "from typing import List, Callable\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895efe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.1\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "print(datasets.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcb618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# CONFIGURATION\n",
    "# ================================\n",
    "\n",
    "# Experiment settings\n",
    "EXPERIMENT_NAME = \"finetuning-34\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Model and LoRA config\n",
    "BASE_MODEL_NAME = \"openai/whisper-large-v3-turbo\" # \"openai/whisper-large-v3\" # \"openai/whisper-large-v2\" \n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"] #, \"k_proj\", \"out_proj\"]\n",
    "\n",
    "# Training config\n",
    "LEARNING_RATE = 2e-6\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 10 #12\n",
    "FP16 = True\n",
    "MAX_LABEL_LENGTH = 256\n",
    "\n",
    "# Dataset config\n",
    "TARGET_SR = 16000\n",
    "AUDIO_COL = \"audio\"\n",
    "TEXT_COL = \"transcription\"\n",
    "TRAIN_NUM_SAMPLES = 8000  # None = full set\n",
    "TEST_NUM_SAMPLES = None   # None = full set\n",
    "EVAL_FROM_TRAIN_PCT = 0  # 0.05 = 5% validation from train\n",
    "\n",
    "# Output files\n",
    "PREDICTIONS_CSV = f\"{EXPERIMENT_NAME}_predictions.csv\"\n",
    "SUMMARY_CSV = f\"{EXPERIMENT_NAME}_summary.csv\"\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633971f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick from: \"commonvoice\", \"fleurs\", \"csalt\" or None\n",
    "# At minimum, train_1 and test_1 must be non-None\n",
    "train_1 = \"commonvoice\"\n",
    "train_2 = \"csalt\"\n",
    "train_3 = None\n",
    "\n",
    "test_1  = \"fleurs\"\n",
    "test_2  = None\n",
    "test_3  = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a64f55",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ba5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text normalization function loaded\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# TEXT NORMALIZATION\n",
    "# ================================\n",
    "\n",
    "# -----------------------------\n",
    "# Core normalization utilities\n",
    "# -----------------------------\n",
    "\n",
    "_ARABIC_DIACRITICS = re.compile(\n",
    "    \"[\"                             # Arabic diacritics range\n",
    "    \"\\u0610-\\u061A\"                 # honorifics, small high\n",
    "    \"\\u064B-\\u065F\"                 # tanwin/harakat\n",
    "    \"\\u0670\"                        # superscript alef\n",
    "    \"\\u06D6-\\u06ED\"                 # Quranic marks\n",
    "    \"]\"\n",
    ")\n",
    "\n",
    "# Zero-width & elongation\n",
    "_ZW_CHARS = re.compile(\"[\\u200B-\\u200F\\u202A-\\u202E\\u2066-\\u2069]\")\n",
    "_KASHIDA  = re.compile(\"\\u0640\")  # tatweel\n",
    "\n",
    "# Arabic presentation forms (NFKC will canonicalize most)\n",
    "def _compat_normalize(s: str) -> str:\n",
    "    # Normalize compatibility forms and spacing\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    # Remove bidi/zero-width and kashida\n",
    "    s = _ZW_CHARS.sub(\"\", s)\n",
    "    s = _KASHIDA.sub(\"\", s)\n",
    "    # Remove diacritics\n",
    "    s = _ARABIC_DIACRITICS.sub(\"\", s)\n",
    "    return s\n",
    "\n",
    "# Map Arabic/Urdu codepoints to a single canonical set often used in Urdu\n",
    "# (Farsi Yeh, Heh goal, etc.)\n",
    "def _canonical_codepoints(s: str) -> str:\n",
    "    # Unify Yeh forms: U+064A (Arabic Yeh), U+06CC (Farsi Yeh) -> choose U+06CC\n",
    "    s = s.replace(\"\\u064A\", \"\\u06CC\")\n",
    "    # Unify Alef Maksura (rare in Urdu) to Farsi Yeh as well (defensive)\n",
    "    s = s.replace(\"\\u0649\", \"\\u06CC\")\n",
    "    # Unify Heh goal variants: ÿ©/Ÿá/€Å/€Ç ‚Üí €Å (U+06C1) when appropriate\n",
    "    # Keep it simple/robust for scoring:\n",
    "    s = s.replace(\"\\u06C0\", \"\\u06C1\")  # heh with hamza above ‚Üí heh goal\n",
    "    # Don't over-aggressively rewrite 'Ÿá' to '€Å' (Arabic heh to Urdu heh goal),\n",
    "    # but we can do a light pass:\n",
    "    s = re.sub(r\"(?<=\\S)\\u0647(?=\\b)\", \"\\u06C1\", s)  # word-final Arabic heh ‚Üí Urdu heh goal\n",
    "    return s\n",
    "\n",
    "# Digits: normalize both Latin and Arabic-Indic to Arabic-Indic (or remove)\n",
    "_ARABIC_INDIC_DIGITS = str.maketrans(\n",
    "    \"0123456789\"\n",
    "    \"Ÿ†Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©\"\n",
    "    \"€∞€±€≤€≥€¥€µ€∂€∑€∏€π\",\n",
    "    \"€∞€±€≤€≥€¥€µ€∂€∑€∏€π\" * 3  # map Latin + Arabic-Indic + Extended to Extended Arabic-Indic\n",
    ")\n",
    "def _normalize_digits(s: str) -> str:\n",
    "    return s.translate(_ARABIC_INDIC_DIGITS)\n",
    "\n",
    "# Remove punctuation & special markers (keep intra-word apostrophes if you want)\n",
    "_PUNCT = re.compile(r\"[^\\w\\s\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]\")  # drop non-Arabic/word chars\n",
    "# Seamless-style disfluencies: remove tokens like #um #uh #laugh\n",
    "_SEAMLESS_DISFL = re.compile(r\"(?<!\\w)#\\w+\")\n",
    "\n",
    "def _strip_punct_and_disfluencies(s: str) -> str:\n",
    "    s = _SEAMLESS_DISFL.sub(\" \", s)\n",
    "    # Convert underscores/odd joins to space first (defensive)\n",
    "    s = s.replace(\"_\", \" \")\n",
    "    s = _PUNCT.sub(\" \", s)\n",
    "    return s\n",
    "\n",
    "def _squash_spaces(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "# -------------------------------------------\n",
    "# Orthographic + token-segmentation variants\n",
    "# -------------------------------------------\n",
    "\n",
    "# Frequent variants noted in paper: \"⁄Üÿß€Å€å€í\" spellings; \"€ÅŸà ⁄Øÿß/€ÅŸà⁄Øÿß\" etc.\n",
    "_VARIANT_CANON = [\n",
    "    # --- ⁄Üÿß€Å€å€í (imperative/necessity) canonicalization ---\n",
    "    # Variants: ⁄Üÿß€Å€åÿ¶€í / ⁄Üÿß⁄æ€å€í / ⁄Üÿß€Åÿ¶€í / ⁄Üÿß€Å€å€ì, etc ‚Üí ⁄Üÿß€Å€å€í\n",
    "    (re.compile(r\"\\b⁄Üÿß€Å€å[ÿ¶€í€ì]\\b\"), \"⁄Üÿß€Å€å€í\"),\n",
    "    (re.compile(r\"\\b⁄Üÿß⁄æ€å[ÿ¶€í€ì]\\b\"), \"⁄Üÿß€Å€å€í\"),\n",
    "    (re.compile(r\"\\b⁄Üÿß€Å[ÿ¶€í€ì]\\b\"), \"⁄Üÿß€Å€å€í\"),\n",
    "    # common stem \"chahie\" unvoweled variants\n",
    "    (re.compile(r\"\\b⁄Üÿß€Å€å?€í\\b\"), \"⁄Üÿß€Å€å€í\"),\n",
    "\n",
    "    # --- €ÅŸà⁄Øÿß family: space-insensitive joining ---\n",
    "    (re.compile(r\"\\b€ÅŸà\\s+⁄Øÿß\\b\"), \"€ÅŸà⁄Øÿß\"),\n",
    "    (re.compile(r\"\\b€ÅŸà\\s+⁄Ø€å\\b\"), \"€ÅŸà⁄Ø€å\"),\n",
    "    (re.compile(r\"\\b€ÅŸà\\s+⁄Ø€í\\b\"), \"€ÅŸà⁄Ø€í\"),\n",
    "    # The reverse (split) hardly needed if we canonicalize to joined forms\n",
    "\n",
    "    # Misc. common merges/splits seen in practice (add as you observe)\n",
    "    (re.compile(r\"\\b⁄©Ÿà ÿ¶€å\\b\"), \"⁄©Ÿàÿ¶€å\"),\n",
    "    (re.compile(r\"\\b⁄©€Å\\b\"), \"⁄©€Å\"),  # noop example; placeholders for future\n",
    "]\n",
    "\n",
    "def _apply_variant_canon(s: str) -> str:\n",
    "    for pat, rep in _VARIANT_CANON:\n",
    "        s = pat.sub(rep, s)\n",
    "    return s\n",
    "\n",
    "# -----------------------------\n",
    "# Public normalizer\n",
    "# -----------------------------\n",
    "def normalize_urdu_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Robust normalizer for Urdu ASR scoring:\n",
    "    - Unicode compatibility & diacritics removal\n",
    "    - Canonical Urdu codepoints (Yeh/Heh goal)\n",
    "    - Remove Seamless-style '#um' disfluencies\n",
    "    - Remove punctuation\n",
    "    - Normalize digits (Latin/Arabic to Eastern Arabic-Indic)\n",
    "    - Canonicalize frequent orthographic variants (⁄Üÿß€Å€å€í, €ÅŸà⁄Øÿß~€ÅŸà ⁄Øÿß)\n",
    "    - Space squashing\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    s = text\n",
    "\n",
    "    # 1) Unicode & presentation forms ‚Üí canonical, drop tatweel/ZW & diacritics\n",
    "    s = _compat_normalize(s)\n",
    "\n",
    "    # 2) Canonical Urdu codepoints\n",
    "    s = _canonical_codepoints(s)\n",
    "\n",
    "    # 3) Disfluencies + punctuation\n",
    "    s = _strip_punct_and_disfluencies(s)\n",
    "\n",
    "    # 4) Digits (optional; or drop all digits if your refs omit numbers)\n",
    "    s = _normalize_digits(s)\n",
    "\n",
    "    # 5) Orthographic canonicalizations & token segmentation fixes\n",
    "    s = _apply_variant_canon(s)\n",
    "\n",
    "    # 6) Collapse spaces\n",
    "    s = _squash_spaces(s)\n",
    "\n",
    "    return s\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Optional: \"lenient\" comparison for WER with variants\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Define lightweight variant generators for lattice expansion on very frequent cases.\n",
    "# Keep these sets tight to avoid combinatorial blow-up.\n",
    "_VARIANT_RULES = {\n",
    "    \"⁄Üÿß€Å€å€í\": {\"⁄Üÿß€Å€å€í\", \"⁄Üÿß€Åÿ¶€í\", \"⁄Üÿß€Å€åÿ¶€í\", \"⁄Üÿß⁄æ€å€í\", \"⁄Üÿß€Å€å€ì\"},\n",
    "    \"€ÅŸà⁄Øÿß\": {\"€ÅŸà⁄Øÿß\", \"€ÅŸà ⁄Øÿß\"},\n",
    "    \"€ÅŸà⁄Ø€å\": {\"€ÅŸà⁄Ø€å\", \"€ÅŸà ⁄Ø€å\"},\n",
    "    \"€ÅŸà⁄Ø€í\": {\"€ÅŸà⁄Ø€í\", \"€ÅŸà ⁄Ø€í\"},\n",
    "}\n",
    "\n",
    "def _expand_variants(tokens: List[str]) -> List[List[str]]:\n",
    "    expanded_per_token = []\n",
    "    for tok in tokens:\n",
    "        expanded_per_token.append(list(_VARIANT_RULES.get(tok, {tok})))\n",
    "    # Cartesian product over tokens to build candidate sequences\n",
    "    return [list(prod) for prod in product(*expanded_per_token)]\n",
    "\n",
    "def generate_lenient_variants(s: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Given a normalized string, produce a small set of alternative strings\n",
    "    accounting for the most common spelling/spacing variants.\n",
    "    \"\"\"\n",
    "    toks = s.split()\n",
    "    seqs = _expand_variants(toks)\n",
    "    return [\" \".join(seq) for seq in seqs]\n",
    "\n",
    "# Example of usage with jiwer:\n",
    "# def lenient_min_wer(ref: str, hyp: str, normalizer: Callable[[str], str] = normalize_urdu_text) -> float:\n",
    "#     r = normalizer(ref)\n",
    "#     h = normalizer(hyp)\n",
    "#     r_cands = generate_lenient_variants(r)\n",
    "#     h_cands = generate_lenient_variants(h)\n",
    "#     # Compute min WER across small lattice of variants\n",
    "#     scores = []\n",
    "#     for rc in r_cands:\n",
    "#         for hc in h_cands:\n",
    "#             scores.append(jiwer_wer(rc, hc))\n",
    "#     return min(scores) if scores else jiwer_wer(r, h)\n",
    "\n",
    "print(\"‚úÖ Text normalization function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b8af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Experiment started: 2026-02-17 15:24:02\n",
      "‚úÖ Using device: cuda\n",
      "‚úÖ GPU: NVIDIA A40\n",
      "‚úÖ Available GPU memory: 47.70 GB\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SETUP\n",
    "# ================================\n",
    "\n",
    "overall_start_time = time.time()\n",
    "print(f\"üïê Experiment started: {datetime.datetime.fromtimestamp(overall_start_time).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(token=\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8d81c",
   "metadata": {},
   "source": [
    "# data laoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97b6beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# DATA LOADING HELPERS\n",
    "# ================================\n",
    "\n",
    "def ensure_audio_and_text(ds, text_keys=(\"transcription\", \"sentence\", \"text\", \"label\")):\n",
    "    \"\"\"Standardize column names to 'audio' and 'transcription'\"\"\"\n",
    "    # Ensure TEXT_COL\n",
    "    if TEXT_COL not in ds.column_names:\n",
    "        for k in text_keys:\n",
    "            if k in ds.column_names:\n",
    "                ds = ds.rename_column(k, TEXT_COL)\n",
    "                break\n",
    "    if TEXT_COL not in ds.column_names:\n",
    "        raise ValueError(\"Could not find transcript column\")\n",
    "\n",
    "    # Ensure AUDIO_COL and cast\n",
    "    if AUDIO_COL not in ds.column_names:\n",
    "        cand = next((c for c in ds.column_names if c.lower() in (\"audio\", \"path\", \"file\")), None)\n",
    "        if cand:\n",
    "            ds = ds.rename_column(cand, AUDIO_COL)\n",
    "    \n",
    "    # Always cast audio to ensure consistent sampling rate and format\n",
    "    ds = ds.cast_column(AUDIO_COL, Audio(sampling_rate=TARGET_SR, decode=True))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def subsample_after_shuffle(ds, n, seed=RANDOM_SEED):\n",
    "    \"\"\"Shuffle and subsample dataset\"\"\"\n",
    "    if n is None or n <= 0 or n >= len(ds):\n",
    "        return ds\n",
    "    return ds.shuffle(seed=seed).select(range(n))\n",
    "\n",
    "def load_csalt_raw():\n",
    "    ds_all = load_dataset(\"urdu-asr/csalt-voice\", token=False)\n",
    "    train_like = ensure_audio_and_text(ds_all[\"validation\"])\n",
    "    return DatasetDict({\"train\": train_like})\n",
    "\n",
    "def load_fleurs_raw():\n",
    "    \"\"\"Load FLEURS Urdu (ur_pk) and merge all splits\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Loading FLEURS (ur_pk)...\")\n",
    "    \n",
    "    try:\n",
    "        ds = load_dataset(\"google/fleurs\", \"ur_pk\")  # ‚ùå remove streaming=True\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load FLEURS ur_pk: {e}\")\n",
    "\n",
    "    merged = {}\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        if split in ds:\n",
    "            merged[split] = ensure_audio_and_text(ds[split])\n",
    "\n",
    "    print(f\"‚úÖ Loaded FLEURS splits: {list(merged.keys())}\")\n",
    "    return DatasetDict(merged)\n",
    "\n",
    "\n",
    "def load_commonvoice_v23_local(src_path):\n",
    "    r\"\"\"\n",
    "    src_path: path to mcv-scripted-ur-v23.0.tar.gz OR to an extracted folder OR directly to ...\\ur\n",
    "    Returns DatasetDict with {train, validation, test} (or train-only via validated.tsv) with AUDIO_COL/TEXT_COL ready.\n",
    "    \"\"\"\n",
    "    # 0) Decide root_dir\n",
    "    if src_path.lower().endswith(\".tar.gz\"):\n",
    "        base_dir = os.path.splitext(os.path.splitext(src_path)[0])[0]  # strip .tar.gz\n",
    "        if not os.path.isdir(base_dir):\n",
    "            print(f\"üì¶ Extracting {os.path.basename(src_path)} ...\")\n",
    "            with tarfile.open(src_path, \"r:gz\") as tf:\n",
    "                tf.extractall(os.path.dirname(src_path))\n",
    "        root_dir = os.path.dirname(src_path)  # search under Downloads after extraction\n",
    "    else:\n",
    "        root_dir = src_path\n",
    "\n",
    "    # 1) Find the folder that has clips/ and tsvs (search any depth)\n",
    "    def has_cv_files(d):\n",
    "        clips_ok = os.path.isdir(os.path.join(d, \"clips\"))\n",
    "        files = {f.lower() for f in os.listdir(d) if os.path.isfile(os.path.join(d, f))}\n",
    "        tsv_ok = (\n",
    "            (\"train.tsv\" in files)\n",
    "            and ((\"dev.tsv\" in files) or (\"validation.tsv\" in files))\n",
    "            and (\"test.tsv\" in files)\n",
    "        ) or (\"validated.tsv\" in files)  # fallback\n",
    "        return clips_ok and tsv_ok\n",
    "\n",
    "    cv_dir = None\n",
    "    if os.path.isdir(root_dir) and has_cv_files(root_dir):\n",
    "        cv_dir = root_dir\n",
    "    else:\n",
    "        for d, dirs, files in os.walk(root_dir):\n",
    "            if has_cv_files(d):\n",
    "                cv_dir = d\n",
    "                break\n",
    "\n",
    "    if cv_dir is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not locate a folder that contains clips/ and train/dev(test)/validated TSVs.\"\n",
    "        )\n",
    "\n",
    "    data_dir  = cv_dir\n",
    "    clips_dir = os.path.join(cv_dir, \"clips\")\n",
    "    print(f\"üìÅ Using data_dir: {data_dir}\")\n",
    "    print(f\"üéß Using clips_dir: {clips_dir}\")\n",
    "\n",
    "    def has_cv_files(d):\n",
    "        return (\n",
    "            os.path.isdir(os.path.join(d, \"clips\")) and\n",
    "            (\n",
    "                (os.path.exists(os.path.join(d, \"train.tsv\")) and\n",
    "                 (os.path.exists(os.path.join(d, \"dev.tsv\")) or os.path.exists(os.path.join(d, \"validation.tsv\"))) and\n",
    "                 os.path.exists(os.path.join(d, \"test.tsv\")))\n",
    "                or os.path.exists(os.path.join(d, \"validated.tsv\"))\n",
    "            )\n",
    "        )\n",
    "\n",
    "    cv_dir = None\n",
    "    if os.path.isdir(root_dir) and has_cv_files(root_dir):\n",
    "        cv_dir = root_dir\n",
    "    else:\n",
    "        for d, _, _ in os.walk(root_dir):\n",
    "            if has_cv_files(d):\n",
    "                cv_dir = d\n",
    "                break\n",
    "    if cv_dir is None:\n",
    "        raise FileNotFoundError(\"Could not locate Common Voice 'ur' folder with clips/ and TSVs.\")\n",
    "\n",
    "    print(f\"üìÅ Using data_dir: {cv_dir}\")\n",
    "    clips_dir = os.path.join(cv_dir, \"clips\")\n",
    "    print(f\"üéß Using clips_dir: {clips_dir}\")\n",
    "\n",
    "    # --- build data_files map (dev vs validation) ---\n",
    "    train_tsv = os.path.join(cv_dir, \"train.tsv\")\n",
    "    dev_tsv   = os.path.join(cv_dir, \"dev.tsv\")\n",
    "    val_tsv   = os.path.join(cv_dir, \"validation.tsv\")\n",
    "    test_tsv  = os.path.join(cv_dir, \"test.tsv\")\n",
    "    validated = os.path.join(cv_dir, \"validated.tsv\")\n",
    "\n",
    "    if os.path.exists(train_tsv) and (os.path.exists(dev_tsv) or os.path.exists(val_tsv)) and os.path.exists(test_tsv):\n",
    "        data_files = {\n",
    "            \"train\": train_tsv,\n",
    "            \"validation\": dev_tsv if os.path.exists(dev_tsv) else val_tsv,\n",
    "            \"test\": test_tsv,\n",
    "        }\n",
    "    elif os.path.exists(validated):\n",
    "        data_files = {\"train\": validated}\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Expected train/dev(or validation)/test TSVs or validated.tsv not found.\")\n",
    "\n",
    "    # --- NEW: force all TSV columns to string to avoid Arrow casting issues ---\n",
    "    # read header columns from a representative TSV (train preferred)\n",
    "    header_probe = data_files.get(\"train\") or next(iter(data_files.values()))\n",
    "    with open(header_probe, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        header = next(reader)\n",
    "    string_features = Features({col: Value(\"string\") for col in header})\n",
    "\n",
    "    # load TSVs with forced string schema\n",
    "    ds_all = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files=data_files,\n",
    "        delimiter=\"\\t\",\n",
    "        features=string_features,   # <- key line\n",
    "    )\n",
    "\n",
    "    # work out text column\n",
    "    first_split = next(iter(ds_all.keys()))\n",
    "    cols = ds_all[first_split].column_names\n",
    "    text_col_name = \"sentence\" if \"sentence\" in cols else (\"text\" if \"text\" in cols else None)\n",
    "    if text_col_name is None:\n",
    "        raise ValueError(f\"No text column found. Columns: {cols}\")\n",
    "\n",
    "    # expand audio paths\n",
    "    audio_col_name = \"path\"\n",
    "    def add_full_audio_path(batch):\n",
    "        batch[audio_col_name] = [os.path.join(clips_dir, fn) for fn in batch[audio_col_name]]\n",
    "        return batch\n",
    "    ds_all = ds_all.map(add_full_audio_path, batched=True)\n",
    "\n",
    "    # filter rows with missing audio\n",
    "    def file_exists(batch):\n",
    "        return [os.path.exists(p) for p in batch[audio_col_name]]\n",
    "    for split in list(ds_all.keys()):\n",
    "        before = len(ds_all[split])\n",
    "        ds_all[split] = ds_all[split].filter(file_exists, batched=True)\n",
    "        after = len(ds_all[split])\n",
    "        print(f\"‚úÖ {split}: kept {after}/{before} rows (dropped {before - after} missing files)\")\n",
    "\n",
    "    # standardize + cast audio\n",
    "    ds_all = ds_all.rename_column(audio_col_name, AUDIO_COL)\n",
    "    if text_col_name != TEXT_COL:\n",
    "        ds_all = ds_all.rename_column(text_col_name, TEXT_COL)\n",
    "    ds_all = ds_all.cast_column(AUDIO_COL, Audio(sampling_rate=TARGET_SR, decode=True))\n",
    "\n",
    "    # final dict\n",
    "    dd = {}\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        if split in ds_all:\n",
    "            dd[split] = ensure_audio_and_text(ds_all[split])\n",
    "    return DatasetDict(dd)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä LOADING DATASETS (dynamic)\n",
      "==================================================\n",
      "Loading CommonVoice...\n",
      "üìÅ Using data_dir: /home/shaider/Downloads/cv-corpus-23.0-2025-09-05/ur\n",
      "üéß Using clips_dir: /home/shaider/Downloads/cv-corpus-23.0-2025-09-05/ur/clips\n",
      "üìÅ Using data_dir: /home/shaider/Downloads/cv-corpus-23.0-2025-09-05/ur\n",
      "üéß Using clips_dir: /home/shaider/Downloads/cv-corpus-23.0-2025-09-05/ur/clips\n",
      "‚úÖ train: kept 7336/7336 rows (dropped 0 missing files)\n",
      "‚úÖ validation: kept 5045/5045 rows (dropped 0 missing files)\n",
      "‚úÖ test: kept 5088/5088 rows (dropped 0 missing files)\n",
      "Loading FLEURS...\n",
      "üîÑ Loading FLEURS (ur_pk)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaider/miniconda3/envs/whisper-lora/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for google/fleurs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google/fleurs\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded FLEURS splits: ['train', 'validation', 'test']\n",
      "Loading CSaLT...\n",
      "\n",
      "--------------------------------------------------\n",
      "üß© Building TRAIN pool from user choices...\n",
      "\n",
      "--------------------------------------------------\n",
      "üß™ Building TEST pool from user choices...\n",
      "\n",
      "==================================================\n",
      "‚úÖ FINAL DATASET SIZES\n",
      "==================================================\n",
      "Train set: 8000 samples\n",
      "Test set:  2675 samples\n",
      "\n",
      "==================================================\n",
      "üìù DATASET SOURCES (for this run)\n",
      "==================================================\n",
      "train_1: commonvoice | train_2: csalt | train_3: -\n",
      "test_1:  fleurs  | test_2:  -  | test_3:  -\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# LOAD AND PREPARE DATASETS (DYNAMIC)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä LOADING DATASETS (dynamic)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1) Load raw DatasetDicts (unchanged)\n",
    "print(\"Loading CommonVoice...\")\n",
    "commonvoice = load_commonvoice_v23_local(r\"/home/shaider/Downloads/cv-corpus-23.0-2025-09-05/ur\")\n",
    "\n",
    "print(\"Loading FLEURS...\")\n",
    "fleurs = load_fleurs_raw()\n",
    "\n",
    "print(\"Loading CSaLT...\")\n",
    "csalt = load_csalt_raw()\n",
    "\n",
    "def merge_all_splits(ds_dict):\n",
    "    \"\"\"\n",
    "    Concatenate all available splits from a DatasetDict.\n",
    "    This mirrors your previous logic (train+validation+test).\n",
    "    \"\"\"\n",
    "    available = [ds_dict[s] for s in [\"train\", \"validation\", \"test\"] if s in ds_dict]\n",
    "    if not available:\n",
    "        raise ValueError(\"No splits found to merge in provided DatasetDict.\")\n",
    "    return concatenate_datasets(available)\n",
    "\n",
    "def safe_select_columns(ds, wanted_cols):\n",
    "    \"\"\"\n",
    "    Select only the columns that actually exist to avoid KeyError\n",
    "    if a source is missing one. (Typically both AUDIO_COL and TEXT_COL exist.)\n",
    "    \"\"\"\n",
    "    keep = [c for c in wanted_cols if c in ds.column_names]\n",
    "    if not keep:\n",
    "        raise ValueError(\n",
    "            f\"None of the requested columns {wanted_cols} are present in {ds.column_names}\"\n",
    "        )\n",
    "    return ds.select_columns(keep)\n",
    "\n",
    "# 2) Build a prepared (merged + column-selected) registry for each dataset name\n",
    "prepared_registry = {\n",
    "    \"commonvoice\": safe_select_columns(merge_all_splits(commonvoice), [AUDIO_COL, TEXT_COL]),\n",
    "    \"fleurs\":      safe_select_columns(merge_all_splits(fleurs),      [AUDIO_COL, TEXT_COL]),\n",
    "    \"csalt\":       safe_select_columns(merge_all_splits(csalt),       [AUDIO_COL, TEXT_COL]),\n",
    "}\n",
    "\n",
    "# 3) Helpers to resolve user choices into a list of prepared datasets\n",
    "def resolve_choice(name: str | None):\n",
    "    if name is None:\n",
    "        return None\n",
    "    key = name.strip().lower()\n",
    "    if key not in prepared_registry:\n",
    "        valid = \", \".join(sorted(prepared_registry.keys()))\n",
    "        raise ValueError(f\"Unknown dataset '{name}'. Valid options: {valid} or None.\")\n",
    "    return prepared_registry[key]\n",
    "\n",
    "def build_pool(*names):\n",
    "    \"\"\"\n",
    "    Given up to three names/None, return a concatenated dataset\n",
    "    of all non-None selections. Requires at least one non-None.\n",
    "    \"\"\"\n",
    "    selected = [resolve_choice(n) for n in names if n is not None]\n",
    "    if not selected:\n",
    "        raise ValueError(\"At least one dataset must be selected to build a pool.\")\n",
    "    if len(selected) == 1:\n",
    "        return selected[0]\n",
    "    return concatenate_datasets(selected)\n",
    "\n",
    "# 4) Resolve TRAIN and TEST pools from the six choices\n",
    "#    (Shuffle + optional subsample mirrors your original behavior)\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"üß© Building TRAIN pool from user choices...\")\n",
    "train_pool = build_pool(train_1, train_2, train_3).shuffle(seed=RANDOM_SEED)\n",
    "\n",
    "# Optional subsampling (disabled if TRAIN_NUM_SAMPLES=None)\n",
    "train_ds = subsample_after_shuffle(train_pool, TRAIN_NUM_SAMPLES, seed=RANDOM_SEED)\n",
    "\n",
    "# Optional: carve validation from train (unchanged)\n",
    "validation_ds = None\n",
    "if EVAL_FROM_TRAIN_PCT > 0.0:\n",
    "    n_eval = int(len(train_ds) * EVAL_FROM_TRAIN_PCT)\n",
    "    if n_eval > 0:\n",
    "        validation_ds = train_ds.select(range(n_eval))\n",
    "        train_ds = train_ds.select(range(n_eval, len(train_ds)))\n",
    "        print(f\"‚úÖ Validation carved from train: {len(validation_ds)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"üß™ Building TEST pool from user choices...\")\n",
    "test_pool = build_pool(test_1, test_2, test_3).shuffle(seed=RANDOM_SEED)\n",
    "\n",
    "# Optional subsampling for test (same helper you already have)\n",
    "test_ds = subsample_after_shuffle(test_pool, TEST_NUM_SAMPLES, seed=RANDOM_SEED)\n",
    "\n",
    "# 5) Summaries\n",
    "def _fmt(x): return x if x is not None else \"-\"\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ FINAL DATASET SIZES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train set: {len(train_ds)} samples\")\n",
    "if validation_ds is not None:\n",
    "    print(f\"Validation set: {len(validation_ds)} samples\")\n",
    "print(f\"Test set:  {len(test_ds)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìù DATASET SOURCES (for this run)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"train_1: {_fmt(train_1)} | train_2: {_fmt(train_2)} | train_3: {_fmt(train_3)}\")\n",
    "print(f\"test_1:  {_fmt(test_1)}  | test_2:  {_fmt(test_2)}  | test_3:  {_fmt(test_3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1122b33",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66318876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üîß MODEL SETUP\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded processor from openai/whisper-large-v3-turbo\n",
      "Loading model in FP16 precision...\n",
      "‚úÖ Configured model for Urdu transcription only (no English translation)\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 13,107,200 || all params: 821,985,280 || trainable%: 1.5946\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# MODEL SETUP\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîß MODEL SETUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load processor\n",
    "processor = WhisperProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "tokenizer = processor.tokenizer\n",
    "feature_extractor = processor.feature_extractor\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Loaded processor from {BASE_MODEL_NAME}\")\n",
    "\n",
    "# Load base model\n",
    "print(f\"Loading model in {'FP16' if FP16 else 'FP32'} precision...\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if FP16 else torch.float32\n",
    ")\n",
    "\n",
    "# ‚úÖ Force Urdu-only transcription mode (no English translation)\n",
    "model.config.forced_decoder_ids = None\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "model.config.language = \"ur\"\n",
    "model.config.task = \"transcribe\"\n",
    "model.generation_config.language = \"ur\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "print(\"‚úÖ Configured model for Urdu transcription only (no English translation)\")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    target_modules=LORA_TARGET_MODULES\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.forward = model.base_model.forward\n",
    "\n",
    "print(\"\\nüìä Trainable Parameters:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5722e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üîÑ PREPROCESSING DATA\n",
      "==================================================\n",
      "‚úÖ Preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DATA PREPROCESSING\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîÑ PREPROCESSING DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"Preprocess audio and text for Whisper\"\"\"\n",
    "    audio = batch[AUDIO_COL]\n",
    "    \n",
    "    # Process audio\n",
    "    inputs = processor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "    \n",
    "    # Process text\n",
    "    tokenized = tokenizer(\n",
    "        batch[TEXT_COL],\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LABEL_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    batch[\"labels\"] = tokenized.input_ids[0]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Preprocess datasets\n",
    "train_ds = train_ds.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_ds.column_names,\n",
    "    desc=\"Preprocessing train set\"\n",
    ")\n",
    "\n",
    "if validation_ds:\n",
    "    validation_ds = validation_ds.map(\n",
    "        prepare_dataset,\n",
    "        remove_columns=validation_ds.column_names,\n",
    "        desc=\"Preprocessing validation set\"\n",
    "    )\n",
    "\n",
    "test_ds = test_ds.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=test_ds.column_names,\n",
    "    desc=\"Preprocessing test set\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üîç PRE-TRAINING WER EVALUATION\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-training evaluation:   0%|          | 0/2675 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Pre-training evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2675/2675 [34:51<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä PRE-TRAINING WER: 0.2283 (22.83%)\n",
      "\n",
      "üîç Sample Normalization Examples:\n",
      "\n",
      "Example 1:\n",
      "  Raw Reference:  €å€Å ŸÖÿßÿ±€å€Å ⁄©€í ŸÜ€å⁄Ü€í Ÿæÿ™ŸÑÿß ÿßŸàÿ± Ÿæ€Åÿß⁄ë€å ÿπŸÑÿßŸÇŸà⁄∫ ⁄©€í ŸÜ€å⁄Ü€í ŸÖŸàŸπÿß €Å€í\n",
      "  Norm Reference: €å€Å ŸÖÿßÿ±€å€Å ⁄©€í ŸÜ€å⁄Ü€í Ÿæÿ™ŸÑÿß ÿßŸàÿ± Ÿæ€Åÿß⁄ë€å ÿπŸÑÿßŸÇŸà⁄∫ ⁄©€í ŸÜ€å⁄Ü€í ŸÖŸàŸπÿß €Å€í\n",
      "  Raw Prediction: €å€Å ŸÖÿßÿ±€åÿß ⁄©€í ŸÜ€å⁄Ü€í Ÿæÿ™ŸÑ€Å ÿßŸàÿ± Ÿæ€Åÿß⁄ë€å ÿπŸÑÿßŸÇŸà⁄∫ ⁄©€í ŸÜ€å⁄Ü€í ŸÖŸàŸπÿß €Å€í\n",
      "  Norm Prediction: €å€Å ŸÖÿßÿ±€åÿß ⁄©€í ŸÜ€å⁄Ü€í Ÿæÿ™ŸÑ€Å ÿßŸàÿ± Ÿæ€Åÿß⁄ë€å ÿπŸÑÿßŸÇŸà⁄∫ ⁄©€í ŸÜ€å⁄Ü€í ŸÖŸàŸπÿß €Å€í\n",
      "\n",
      "Example 2:\n",
      "  Raw Reference:  ÿ¥€ÅŸÜÿ¥ÿß€Å ÿ¢⁄Øÿ≥Ÿπÿ≥ ⁄©€í ÿ∞ÿ±€åÿπ€í ⁄àÿß⁄©Ÿπÿ±Ÿà⁄∫ ⁄©€å ÿ®⁄æÿ±ÿ™€å ÿ¥ÿ±Ÿàÿπ €ÅŸàÿ¶€å ÿßŸàÿ± ÿßŸÜ€ÅŸà⁄∫ ŸÜ€í ÿ¨ŸÜ⁄ØŸà⁄∫ ⁄©€í ÿ®ÿπÿØ ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ €ÅŸàŸÜ€í ŸàÿßŸÑÿß ÿ≥ÿ® ÿ≥€í Ÿæ\n",
      "  Norm Reference: ÿ¥€ÅŸÜÿ¥ÿß€Å ÿ¢⁄Øÿ≥Ÿπÿ≥ ⁄©€í ÿ∞ÿ±€åÿπ€í ⁄àÿß⁄©Ÿπÿ±Ÿà⁄∫ ⁄©€å ÿ®⁄æÿ±ÿ™€å ÿ¥ÿ±Ÿàÿπ €ÅŸàÿ¶€å ÿßŸàÿ± ÿßŸÜ€ÅŸà⁄∫ ŸÜ€í ÿ¨ŸÜ⁄ØŸà⁄∫ ⁄©€í ÿ®ÿπÿØ ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ €ÅŸàŸÜ€í ŸàÿßŸÑÿß ÿ≥ÿ® ÿ≥€í Ÿæ\n",
      "  Raw Prediction: ÿ¥€åŸÜ ÿ¥ÿß€Å ÿ¢⁄Øÿ≥ ÿ™ÿ±ÿ≥ ⁄©€í ÿ∞ÿ±€åÿπ€í ⁄àÿß⁄©Ÿπÿ±Ÿà⁄∫ ⁄©€å ÿ®ÿ±ÿ™€å ÿ¥ÿ±Ÿàÿπ €ÅŸàÿ¶€å ÿßŸÜ€ÅŸà⁄∫ ŸÜ€í ÿ¨ŸÖÿπŸà⁄∫ ⁄©€í ÿ®ÿπÿØ ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ €ÅŸàŸÜ€í ŸàÿßŸÑÿß ÿ≥ÿ® ÿ≥€í Ÿæ€ÅŸÑ\n",
      "  Norm Prediction: ÿ¥€åŸÜ ÿ¥ÿß€Å ÿ¢⁄Øÿ≥ ÿ™ÿ±ÿ≥ ⁄©€í ÿ∞ÿ±€åÿπ€í ⁄àÿß⁄©Ÿπÿ±Ÿà⁄∫ ⁄©€å ÿ®ÿ±ÿ™€å ÿ¥ÿ±Ÿàÿπ €ÅŸàÿ¶€å ÿßŸÜ€ÅŸà⁄∫ ŸÜ€í ÿ¨ŸÖÿπŸà⁄∫ ⁄©€í ÿ®ÿπÿØ ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ €ÅŸàŸÜ€í ŸàÿßŸÑÿß ÿ≥ÿ® ÿ≥€í Ÿæ€ÅŸÑ\n",
      "\n",
      "Example 3:\n",
      "  Raw Reference:  €ÅÿßŸÜ⁄Ø ⁄©ÿßŸÜ⁄Ø ⁄©ÿß ÿ®€Åÿ™ÿ±€åŸÜ ŸÜÿ∏ÿßÿ±€Å ⁄©ÿ±ŸÜ€í ⁄©€í ŸÑÿ¶€í ÿ¨ÿ≤€åÿ±€Å ⁄©Ÿà ⁄Ü⁄æŸà⁄ëŸà ÿßŸàÿ± ÿßÿ≥ ⁄©€í ÿ®ŸÑŸÖŸÇÿßÿ®ŸÑ ⁄©ŸàŸÑŸàŸÜ ŸàÿßŸπÿ± ŸÅÿ±ŸÜŸπ Ÿæÿ± ÿ¨ÿßÿ§\n",
      "  Norm Reference: €ÅÿßŸÜ⁄Ø ⁄©ÿßŸÜ⁄Ø ⁄©ÿß ÿ®€Åÿ™ÿ±€åŸÜ ŸÜÿ∏ÿßÿ±€Å ⁄©ÿ±ŸÜ€í ⁄©€í ŸÑÿ¶€í ÿ¨ÿ≤€åÿ±€Å ⁄©Ÿà ⁄Ü⁄æŸà⁄ëŸà ÿßŸàÿ± ÿßÿ≥ ⁄©€í ÿ®ŸÑŸÖŸÇÿßÿ®ŸÑ ⁄©ŸàŸÑŸàŸÜ ŸàÿßŸπÿ± ŸÅÿ±ŸÜŸπ Ÿæÿ± ÿ¨ÿßÿ§\n",
      "  Raw Prediction: €ÅÿßŸÜ⁄Ø ⁄©ŸàŸÜ⁄Ø ⁄©ÿß ÿ®€Åÿ™ÿ±€åŸÜ ŸÜÿ∏ÿßÿ±€Å ⁄©ÿ±ŸÜ€í ⁄©€í ŸÑ€å€í ÿ¨ÿ≤€åÿ±€Å ⁄©Ÿà ⁄Ü⁄æŸà⁄ëŸà ÿßŸàÿ± ÿßÿ≥ ⁄©€í ÿ®ÿßŸÑŸÖŸÇÿßÿ®ŸÑ ⁄©ŸàŸÑŸàŸÜ Ÿàÿßÿ™ÿ± ŸÅÿ±ŸÜŸπ Ÿæÿ± ÿ¨ÿßÿ§\n",
      "  Norm Prediction: €ÅÿßŸÜ⁄Ø ⁄©ŸàŸÜ⁄Ø ⁄©ÿß ÿ®€Åÿ™ÿ±€åŸÜ ŸÜÿ∏ÿßÿ±€Å ⁄©ÿ±ŸÜ€í ⁄©€í ŸÑ€å€í ÿ¨ÿ≤€åÿ±€Å ⁄©Ÿà ⁄Ü⁄æŸà⁄ëŸà ÿßŸàÿ± ÿßÿ≥ ⁄©€í ÿ®ÿßŸÑŸÖŸÇÿßÿ®ŸÑ ⁄©ŸàŸÑŸàŸÜ Ÿàÿßÿ™ÿ± ŸÅÿ±ŸÜŸπ Ÿæÿ± ÿ¨ÿßÿ§\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# PRE-TRAINING EVALUATION\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîç PRE-TRAINING WER EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def evaluate_model(model, test_dataset, device, desc=\"Evaluating\"):\n",
    "    \"\"\"Evaluate model and return WER metrics\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    predictions_raw = []  # Store raw predictions for debugging\n",
    "    references_raw = []   # Store raw references for debugging\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(test_dataset, desc=desc):\n",
    "            input_features = torch.tensor(sample[\"input_features\"]).unsqueeze(0).to(device)\n",
    "            \n",
    "            if FP16:\n",
    "                input_features = input_features.half()\n",
    "            \n",
    "            pred_ids = model.generate(input_features=input_features)\n",
    "            pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)[0].strip()\n",
    "            \n",
    "            # Get reference from the preprocessed labels\n",
    "            label_ids = sample[\"labels\"]\n",
    "            # Remove padding tokens\n",
    "            label_ids = [id for id in label_ids if id != tokenizer.pad_token_id]\n",
    "            label_str = tokenizer.decode(label_ids, skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Store raw versions\n",
    "            predictions_raw.append(pred_str)\n",
    "            references_raw.append(label_str)\n",
    "            \n",
    "            # *** APPLY TEXT NORMALIZATION HERE ***\n",
    "            pred_str_normalized = normalize_urdu_text(pred_str)\n",
    "            label_str_normalized = normalize_urdu_text(label_str)\n",
    "            \n",
    "            predictions.append(pred_str_normalized)\n",
    "            references.append(label_str_normalized)\n",
    "    \n",
    "    # Calculate WER on normalized text\n",
    "    sample_wers = [jiwer_wer(ref, pred) for ref, pred in zip(references, predictions)]\n",
    "    overall_wer = np.mean(sample_wers)\n",
    "    \n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "        \"predictions_raw\": predictions_raw,  # Include raw for debugging\n",
    "        \"references_raw\": references_raw,\n",
    "        \"sample_wers\": sample_wers,\n",
    "        \"overall_wer\": overall_wer\n",
    "    }\n",
    "\n",
    "# Evaluate before fine-tuning\n",
    "pre_results = evaluate_model(model, test_ds, device, desc=\"Pre-training evaluation\")\n",
    "pre_training_wer = pre_results[\"overall_wer\"]\n",
    "\n",
    "print(f\"\\nüìä PRE-TRAINING WER: {pre_training_wer:.4f} ({pre_training_wer*100:.2f}%)\")\n",
    "\n",
    "# Optional: Show some examples to verify normalization is working\n",
    "print(\"\\nüîç Sample Normalization Examples:\")\n",
    "for i in range(min(3, len(pre_results[\"predictions\"]))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Raw Reference:  {pre_results['references_raw'][i][:100]}\")\n",
    "    print(f\"  Norm Reference: {pre_results['references'][i][:100]}\")\n",
    "    print(f\"  Raw Prediction: {pre_results['predictions_raw'][i][:100]}\")\n",
    "    print(f\"  Norm Prediction: {pre_results['predictions'][i][:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb312c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèãÔ∏è TRAINING SETUP\n",
      "==================================================\n",
      "Optimizer: AdamW (lr=2e-06)\n",
      "Batch size: 8\n",
      "Total batches per epoch: 1000\n",
      "Mixed precision (FP16): True\n",
      "Total training steps: 10000\n",
      "Warmup steps: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tmp/ipykernel_17023/3470551819.py:52: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if FP16 and torch.cuda.is_available() else None\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# TRAINING SETUP\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèãÔ∏è TRAINING SETUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for DataLoader\"\"\"\n",
    "    input_feats = torch.stack([\n",
    "        torch.tensor(item[\"input_features\"], dtype=torch.float32)\n",
    "        for item in batch\n",
    "    ])\n",
    "    \n",
    "    label_tensors = pad_sequence(\n",
    "        [torch.tensor(item[\"labels\"], dtype=torch.long) for item in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_features\": input_feats,\n",
    "        \"labels\": label_tensors\n",
    "    }\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "num_training_steps = len(train_loader) * NUM_EPOCHS\n",
    "num_warmup_steps = int(0.1 * num_training_steps)   # or fixed: 500, 1000, etc.\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Create scheduler\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    "    last_epoch=-1   # important if you ever resume training\n",
    ")\n",
    "\n",
    "# Setup gradient scaler for FP16\n",
    "scaler = torch.cuda.amp.GradScaler() if FP16 and torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"Optimizer: AdamW (lr={LEARNING_RATE})\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Total batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Mixed precision (FP16): {FP16}\")\n",
    "print(f\"Total training steps: {num_training_steps}\")\n",
    "print(f\"Warmup steps: {num_warmup_steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58936956",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633e0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üöÄ STARTING TRAINING\n",
      "==================================================\n",
      "\n",
      "üéØ Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1000 [00:00<?, ?it/s]/data/tmp/ipykernel_17023/4241112303.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [37:36<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 complete ‚Äî Avg Loss: 0.8881\n",
      "Epoch 1 finished ‚Äî Avg Loss: 0.8881 | LR: 2.00e-06\n",
      "\n",
      "üéØ Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [37:21<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 complete ‚Äî Avg Loss: 0.1580\n",
      "Epoch 2 finished ‚Äî Avg Loss: 0.1580 | LR: 1.78e-06\n",
      "\n",
      "üéØ Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [37:14<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3 complete ‚Äî Avg Loss: 0.1117\n",
      "Epoch 3 finished ‚Äî Avg Loss: 0.1117 | LR: 1.56e-06\n",
      "\n",
      "üéØ Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [37:12<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 4 complete ‚Äî Avg Loss: 0.0766\n",
      "Epoch 4 finished ‚Äî Avg Loss: 0.0766 | LR: 1.33e-06\n",
      "\n",
      "üéØ Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [37:20<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 5 complete ‚Äî Avg Loss: 0.0565\n",
      "Epoch 5 finished ‚Äî Avg Loss: 0.0565 | LR: 1.11e-06\n",
      "\n",
      "üéØ Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [37:18<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 6 complete ‚Äî Avg Loss: 0.0529\n",
      "Epoch 6 finished ‚Äî Avg Loss: 0.0529 | LR: 8.89e-07\n",
      "\n",
      "üéØ Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [37:15<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 7 complete ‚Äî Avg Loss: 0.0510\n",
      "Epoch 7 finished ‚Äî Avg Loss: 0.0510 | LR: 6.67e-07\n",
      "\n",
      "üéØ Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [37:13<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 8 complete ‚Äî Avg Loss: 0.0497\n",
      "Epoch 8 finished ‚Äî Avg Loss: 0.0497 | LR: 4.44e-07\n",
      "\n",
      "üéØ Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [37:10<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 9 complete ‚Äî Avg Loss: 0.0487\n",
      "Epoch 9 finished ‚Äî Avg Loss: 0.0487 | LR: 2.22e-07\n",
      "\n",
      "üéØ Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [37:06<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 10 complete ‚Äî Avg Loss: 0.0481\n",
      "Epoch 10 finished ‚Äî Avg Loss: 0.0481 | LR: 0.00e+00\n",
      "\n",
      "‚úÖ Training complete! Duration: 6:12:50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# TRAINING\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "train_start_time = time.time()\n",
    "model.train()\n",
    "validation_wers = []  # <---- ADD THIS\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    print(f\"\\nüéØ Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        try:\n",
    "            input_feats = batch[\"input_features\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            if FP16 and scaler is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(input_features=input_feats, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                if FP16:\n",
    "                    input_feats = input_feats.half()\n",
    "\n",
    "                outputs = model(input_features=input_feats, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # ‚îÄ‚îÄ Important: scheduler step goes here ‚îÄ‚îÄ\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing batch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} complete ‚Äî Avg Loss: {avg_loss:.4f}\")\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1} finished ‚Äî Avg Loss: {avg_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Validation if available\n",
    "    if validation_ds:\n",
    "        val_results = evaluate_model(model, validation_ds, device, desc=\"Validation\")\n",
    "        val_wer = round(val_results[\"overall_wer\"], 4)\n",
    "        print(f\"üîé Validation WER: {val_wer:.4f}\")\n",
    "        validation_wers.append(val_wer)  # <---- ADD THIS\n",
    "        model.train()  # Back to training mode\n",
    "\n",
    "train_end_time = time.time()\n",
    "train_duration_secs = int(train_end_time - train_start_time)\n",
    "train_duration_hms = str(datetime.timedelta(seconds=train_duration_secs))\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Duration: {train_duration_hms}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949f2d6",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c5464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìè POST-TRAINING WER EVALUATION\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Post-training evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2675/2675 [35:24<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä POST-TRAINING WER: 0.2287 (22.87%)\n",
      "\n",
      "üéâ WER IMPROVEMENT: -0.0004 (-0.18%)\n",
      "   Pre-training:  0.2283\n",
      "   Post-training: 0.2287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# POST-TRAINING EVALUATION\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìè POST-TRAINING WER EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Evaluate after fine-tuning\n",
    "post_results = evaluate_model(model, test_ds, device, desc=\"Post-training evaluation\")\n",
    "post_training_wer = post_results[\"overall_wer\"]\n",
    "\n",
    "print(f\"\\nüìä POST-TRAINING WER: {post_training_wer:.4f} ({post_training_wer*100:.2f}%)\")\n",
    "\n",
    "# Calculate improvement\n",
    "wer_improvement = pre_training_wer - post_training_wer\n",
    "wer_improvement_pct = (wer_improvement / pre_training_wer) * 100\n",
    "\n",
    "print(f\"\\nüéâ WER IMPROVEMENT: {wer_improvement:.4f} ({wer_improvement_pct:.2f}%)\")\n",
    "print(f\"   Pre-training:  {pre_training_wer:.4f}\")\n",
    "print(f\"   Post-training: {post_training_wer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "069c8696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üî¨ DECODER SWEEP: Evaluating generation settings\n",
      "==================================================\n",
      "üìä Using 500 samples for decoder sweep\n",
      "\n",
      "‚öôÔ∏è Building smart decoder grid...\n",
      "üß™ Total decoder configs generated: 73\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DECODER SWEEP\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üî¨ DECODER SWEEP: Evaluating generation settings\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Subsample test set for faster sweep\n",
    "SWEEP_SAMPLE_SIZE = min(500, len(test_ds))\n",
    "sweep_test_ds = test_ds.shuffle(seed=RANDOM_SEED).select(range(SWEEP_SAMPLE_SIZE))\n",
    "print(f\"üìä Using {SWEEP_SAMPLE_SIZE} samples for decoder sweep\")\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# --- keep this grid small / meaningful; remove params not accepted by generate()\n",
    "param_grid = {\n",
    "    \"num_beams\": [1, 2, 3],                   # keep small for quick test (increase on A40)\n",
    "    \"length_penalty\": [None, 0.9, 1.0, 1.1],\n",
    "    \"no_repeat_ngram_size\": [None, 2, 3],\n",
    "    \"repetition_penalty\": [None, 1.05, 1.1],\n",
    "    \"do_sample\": [False],                     # deterministic for ASR\n",
    "    \"max_new_tokens\": [225],\n",
    "}\n",
    "\n",
    "# Build configs, normalize and dedupe\n",
    "decoder_configs = []\n",
    "seen = set()\n",
    "idx = 0\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Building smart decoder grid...\")\n",
    "\n",
    "for combo in product(*param_grid.values()):\n",
    "    cfg = dict(zip(param_grid.keys(), combo))\n",
    "\n",
    "    # Normalize greedy: if num_beams==1, strip beam-only params\n",
    "    if cfg[\"num_beams\"] == 1:\n",
    "        cfg[\"length_penalty\"] = None\n",
    "        cfg[\"no_repeat_ngram_size\"] = None\n",
    "        cfg[\"repetition_penalty\"] = None\n",
    "\n",
    "    # Skip logically invalid combos (no-repeat only makes sense with beams>1)\n",
    "    if cfg[\"num_beams\"] == 1 and cfg[\"no_repeat_ngram_size\"] is not None:\n",
    "        continue\n",
    "    if cfg[\"do_sample\"] and cfg[\"num_beams\"] > 1:\n",
    "        continue\n",
    "\n",
    "    # Create deterministic signature for deduplication (ignore name)\n",
    "    sig_items = tuple(sorted((k, v) for k, v in cfg.items()))\n",
    "    if sig_items in seen:\n",
    "        continue\n",
    "    seen.add(sig_items)\n",
    "\n",
    "    cfg[\"name\"] = f\"cfg_{idx}\"\n",
    "    decoder_configs.append(cfg)\n",
    "    idx += 1\n",
    "\n",
    "print(f\"üß™ Total decoder configs generated: {len(decoder_configs)}\")\n",
    "\n",
    "# Helper: sanitize gen kwargs (remove None / pandas NA and convert numpy types)\n",
    "def sanitize_gen_kwargs(cfg):\n",
    "    gen = {}\n",
    "    for k, v in cfg.items():\n",
    "        if k == \"name\":\n",
    "            continue\n",
    "        # drop None/NaN\n",
    "        if v is None:\n",
    "            continue\n",
    "        if (isinstance(v, float) and np.isnan(v)) or pd.isna(v):\n",
    "            continue\n",
    "        # convert numpy scalar types to native python\n",
    "        if isinstance(v, (np.integer, np.int64)):\n",
    "            v = int(v)\n",
    "        elif isinstance(v, (np.floating, np.float64)):\n",
    "            v = float(v)\n",
    "        elif isinstance(v, (np.bool_,)):\n",
    "            v = bool(v)\n",
    "        gen[k] = v\n",
    "    return gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0694a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# EVALUATION CODE (per-config)\n",
    "# ===============================\n",
    "def evaluate_decoder_config(model, test_dataset, device, config, desc=\"\"):\n",
    "    model.eval()\n",
    "    predictions, references, predictions_raw, references_raw = [], [], [], []\n",
    "\n",
    "    gen_kwargs = sanitize_gen_kwargs(config)\n",
    "\n",
    "    # default\n",
    "    if \"num_beams\" not in gen_kwargs:\n",
    "        gen_kwargs[\"num_beams\"] = 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(test_dataset, desc=desc, leave=False):\n",
    "            # ensure tensor on CPU->GPU\n",
    "            input_features = torch.tensor(sample[\"input_features\"]).unsqueeze(0).to(device)\n",
    "            if FP16:\n",
    "                input_features = input_features.half()\n",
    "\n",
    "            pred_ids = model.generate(input_features=input_features, **gen_kwargs)\n",
    "            pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "            label_ids = [id for id in sample[\"labels\"] if id != tokenizer.pad_token_id]\n",
    "            label_str = tokenizer.decode(label_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "            # store raw\n",
    "            predictions_raw.append(pred_str)\n",
    "            references_raw.append(label_str)\n",
    "\n",
    "            # normalize for scoring\n",
    "            pred_str_norm = normalize_urdu_text(pred_str)\n",
    "            label_str_norm = normalize_urdu_text(label_str)\n",
    "\n",
    "            predictions.append(pred_str_norm)\n",
    "            references.append(label_str_norm)\n",
    "\n",
    "    sample_wers = [jiwer_wer(r, p) for r, p in zip(references, predictions)]\n",
    "    mean_wer = float(np.mean(sample_wers))\n",
    "    return {\n",
    "        \"overall_wer\": mean_wer,\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "        \"predictions_raw\": predictions_raw,\n",
    "        \"references_raw\": references_raw,\n",
    "        \"sample_wers\": sample_wers,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc216b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Running decoder sweep...\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2203\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2192\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2196\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2197\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2940\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2946\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2941\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2462\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2464\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2462\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2192\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2195\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2197\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2936\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2942\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2938\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2462\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2462\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2461\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2192\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2196\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2197\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2940\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2946\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2941\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2462\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2464\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2462\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2192\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2195\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2197\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2940\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2946\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2942\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2463\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2465\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2465\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2188\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2191\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2194\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2948\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2967\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2962\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2463\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2463\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2463\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2187\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2191\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2194\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2954\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2971\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2967\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2461\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2463\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2463\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2188\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2191\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2194\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2948\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2967\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2962\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2463\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2463\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2463\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2188\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2189\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2193\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2938\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2957\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2951\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2463\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2463\n",
      "\n",
      "‚öôÔ∏è Testing: cfg_72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ WER: 0.2462\n",
      "\n",
      "==================================================\n",
      "üìä DECODER SWEEP RESULTS (sorted by WER)\n",
      "==================================================\n",
      "config_name    wer  num_beams  length_penalty  no_repeat_ngram_size  repetition_penalty  do_sample  max_new_tokens\n",
      "     cfg_46 0.2187          3             0.9                   NaN                 NaN      False             225\n",
      "     cfg_37 0.2188          3             NaN                   NaN                 NaN      False             225\n",
      "     cfg_64 0.2188          3             1.1                   NaN                 NaN      False             225\n",
      "     cfg_55 0.2188          3             1.0                   NaN                 NaN      False             225\n",
      "     cfg_65 0.2189          3             1.1                   NaN                1.05      False             225\n",
      "     cfg_47 0.2191          3             0.9                   NaN                1.05      False             225\n",
      "     cfg_38 0.2191          3             NaN                   NaN                1.05      False             225\n",
      "     cfg_56 0.2191          3             1.0                   NaN                1.05      False             225\n",
      "     cfg_19 0.2192          2             1.0                   NaN                 NaN      False             225\n",
      "     cfg_10 0.2192          2             0.9                   NaN                 NaN      False             225\n",
      "     cfg_28 0.2192          2             1.1                   NaN                 NaN      False             225\n",
      "      cfg_1 0.2192          2             NaN                   NaN                 NaN      False             225\n",
      "     cfg_66 0.2193          3             1.1                   NaN                1.10      False             225\n",
      "     cfg_48 0.2194          3             0.9                   NaN                1.10      False             225\n",
      "     cfg_39 0.2194          3             NaN                   NaN                1.10      False             225\n",
      "     cfg_57 0.2194          3             1.0                   NaN                1.10      False             225\n",
      "     cfg_29 0.2195          2             1.1                   NaN                1.05      False             225\n",
      "     cfg_11 0.2195          2             0.9                   NaN                1.05      False             225\n",
      "      cfg_2 0.2196          2             NaN                   NaN                1.05      False             225\n",
      "     cfg_20 0.2196          2             1.0                   NaN                1.05      False             225\n",
      "     cfg_12 0.2197          2             0.9                   NaN                1.10      False             225\n",
      "      cfg_3 0.2197          2             NaN                   NaN                1.10      False             225\n",
      "     cfg_30 0.2197          2             1.1                   NaN                1.10      False             225\n",
      "     cfg_21 0.2197          2             1.0                   NaN                1.10      False             225\n",
      "      cfg_0 0.2203          1             NaN                   NaN                 NaN      False             225\n",
      "     cfg_18 0.2461          2             0.9                   3.0                1.10      False             225\n",
      "     cfg_52 0.2461          3             0.9                   3.0                 NaN      False             225\n",
      "     cfg_72 0.2462          3             1.1                   3.0                1.10      False             225\n",
      "     cfg_16 0.2462          2             0.9                   3.0                 NaN      False             225\n",
      "     cfg_25 0.2462          2             1.0                   3.0                 NaN      False             225\n",
      "     cfg_27 0.2462          2             1.0                   3.0                1.10      False             225\n",
      "     cfg_17 0.2462          2             0.9                   3.0                1.05      False             225\n",
      "      cfg_9 0.2462          2             NaN                   3.0                1.10      False             225\n",
      "      cfg_7 0.2462          2             NaN                   3.0                 NaN      False             225\n",
      "     cfg_53 0.2463          3             0.9                   3.0                1.05      False             225\n",
      "     cfg_54 0.2463          3             0.9                   3.0                1.10      False             225\n",
      "     cfg_45 0.2463          3             NaN                   3.0                1.10      False             225\n",
      "     cfg_71 0.2463          3             1.1                   3.0                1.05      False             225\n",
      "     cfg_34 0.2463          2             1.1                   3.0                 NaN      False             225\n",
      "     cfg_61 0.2463          3             1.0                   3.0                 NaN      False             225\n",
      "     cfg_62 0.2463          3             1.0                   3.0                1.05      False             225\n",
      "     cfg_63 0.2463          3             1.0                   3.0                1.10      False             225\n",
      "     cfg_70 0.2463          3             1.1                   3.0                 NaN      False             225\n",
      "     cfg_43 0.2463          3             NaN                   3.0                 NaN      False             225\n",
      "     cfg_44 0.2463          3             NaN                   3.0                1.05      False             225\n",
      "     cfg_26 0.2464          2             1.0                   3.0                1.05      False             225\n",
      "      cfg_8 0.2464          2             NaN                   3.0                1.05      False             225\n",
      "     cfg_36 0.2465          2             1.1                   3.0                1.10      False             225\n",
      "     cfg_35 0.2465          2             1.1                   3.0                1.05      False             225\n",
      "     cfg_13 0.2936          2             0.9                   2.0                 NaN      False             225\n",
      "     cfg_15 0.2938          2             0.9                   2.0                1.10      False             225\n",
      "     cfg_67 0.2938          3             1.1                   2.0                 NaN      False             225\n",
      "     cfg_31 0.2940          2             1.1                   2.0                 NaN      False             225\n",
      "     cfg_22 0.2940          2             1.0                   2.0                 NaN      False             225\n",
      "      cfg_4 0.2940          2             NaN                   2.0                 NaN      False             225\n",
      "     cfg_24 0.2941          2             1.0                   2.0                1.10      False             225\n",
      "      cfg_6 0.2941          2             NaN                   2.0                1.10      False             225\n",
      "     cfg_33 0.2942          2             1.1                   2.0                1.10      False             225\n",
      "     cfg_14 0.2942          2             0.9                   2.0                1.05      False             225\n",
      "     cfg_23 0.2946          2             1.0                   2.0                1.05      False             225\n",
      "      cfg_5 0.2946          2             NaN                   2.0                1.05      False             225\n",
      "     cfg_32 0.2946          2             1.1                   2.0                1.05      False             225\n",
      "     cfg_58 0.2948          3             1.0                   2.0                 NaN      False             225\n",
      "     cfg_40 0.2948          3             NaN                   2.0                 NaN      False             225\n",
      "     cfg_69 0.2951          3             1.1                   2.0                1.10      False             225\n",
      "     cfg_49 0.2954          3             0.9                   2.0                 NaN      False             225\n",
      "     cfg_68 0.2957          3             1.1                   2.0                1.05      False             225\n",
      "     cfg_60 0.2962          3             1.0                   2.0                1.10      False             225\n",
      "     cfg_42 0.2962          3             NaN                   2.0                1.10      False             225\n",
      "     cfg_41 0.2967          3             NaN                   2.0                1.05      False             225\n",
      "     cfg_51 0.2967          3             0.9                   2.0                1.10      False             225\n",
      "     cfg_59 0.2967          3             1.0                   2.0                1.05      False             225\n",
      "     cfg_50 0.2971          3             0.9                   2.0                1.05      False             225\n",
      "\n",
      "==================================================\n",
      "üèÜ BEST DECODER CONFIGURATION\n",
      "==================================================\n",
      "Config: cfg_46\n",
      "WER: 0.2187\n",
      "Parameters:\n",
      "  - config_name: cfg_46\n",
      "  - wer: 0.2187\n",
      "  - num_beams: 3\n",
      "  - length_penalty: 0.9\n",
      "  - do_sample: False\n",
      "  - max_new_tokens: 225\n",
      "\n",
      "üìà Improvement over greedy decoding:\n",
      "   Greedy WER: 0.2203\n",
      "   Best WER: 0.2187\n",
      "   Improvement: 0.0016 (0.71%)\n",
      "\n",
      "üéâ Decoder sweep complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# RUN SWEEP\n",
    "# ===============================\n",
    "decoder_results = []\n",
    "decoder_details = {}  # keep per-config detailed outputs if you want\n",
    "\n",
    "print(\"\\nüîÑ Running decoder sweep...\")\n",
    "for i, config in enumerate(decoder_configs, 1):\n",
    "    config_name = config[\"name\"]\n",
    "    desc = f\"[{i}/{len(decoder_configs)}] {config_name}\"\n",
    "    print(f\"\\n‚öôÔ∏è Testing: {config_name}\")\n",
    "\n",
    "    try:\n",
    "        out = evaluate_decoder_config(model, sweep_test_ds, device, config, desc=desc)\n",
    "        wer_score = out[\"overall_wer\"]\n",
    "\n",
    "        result = {\n",
    "            \"config_name\": config_name,\n",
    "            \"wer\": round(wer_score, 4),\n",
    "            **{k: v for k, v in config.items() if k != \"name\"}\n",
    "        }\n",
    "        decoder_results.append(result)\n",
    "        decoder_details[config_name] = out  # store details\n",
    "\n",
    "        print(f\"   ‚úÖ WER: {wer_score:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# Process results\n",
    "if not decoder_results:\n",
    "    raise RuntimeError(\"Decoder sweep yielded no successful results.\")\n",
    "\n",
    "decoder_df = pd.DataFrame(decoder_results).sort_values(\"wer\").reset_index(drop=True)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä DECODER SWEEP RESULTS (sorted by WER)\")\n",
    "print(\"=\"*50)\n",
    "print(decoder_df.to_string(index=False))\n",
    "\n",
    "best_row = decoder_df.iloc[0]\n",
    "best_cfg = best_row  # pandas Series\n",
    "\n",
    "# Build best_gen_kwargs (cleaned)\n",
    "best_gen_kwargs = sanitize_gen_kwargs(best_cfg.to_dict())\n",
    "\n",
    "# Get detailed optimized_results from decoder_details\n",
    "best_name = best_cfg[\"config_name\"]\n",
    "optimized_results = decoder_details.get(best_name, None)\n",
    "if optimized_results is None:\n",
    "    # fallback: re-evaluate and capture full outputs\n",
    "    optimized_results = evaluate_decoder_config(model, test_ds, device, best_cfg.to_dict(), desc=\"Recompute best config\")\n",
    "\n",
    "optimized_wer = optimized_results[\"overall_wer\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèÜ BEST DECODER CONFIGURATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Config: {best_name}\")\n",
    "print(f\"WER: {optimized_wer:.4f}\")\n",
    "print(\"Parameters:\")\n",
    "for k, v in best_gen_kwargs.items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "\n",
    "# comparison to greedy (robust)\n",
    "greedy_row = decoder_df[decoder_df.get(\"num_beams\") == 1]\n",
    "if not greedy_row.empty:\n",
    "    greedy_wer = greedy_row.iloc[0][\"wer\"]\n",
    "    sweep_improvement = greedy_wer - optimized_wer\n",
    "    sweep_improvement_pct = (sweep_improvement / greedy_wer) * 100 if greedy_wer != 0 else 0.0\n",
    "    print(\"\\nüìà Improvement over greedy decoding:\")\n",
    "    print(f\"   Greedy WER: {greedy_wer:.4f}\")\n",
    "    print(f\"   Best WER: {optimized_wer:.4f}\")\n",
    "    print(f\"   Improvement: {sweep_improvement:.4f} ({sweep_improvement_pct:.2f}%)\")\n",
    "\n",
    "# Build decoder_sweep_data for saving/summary\n",
    "decoder_improvement = post_training_wer - optimized_wer\n",
    "decoder_improvement_pct = (decoder_improvement / post_training_wer) * 100 if post_training_wer != 0 else 0.0\n",
    "\n",
    "decoder_sweep_data = {\n",
    "    \"best_decoder_config\": best_name,\n",
    "    \"best_decoder_config_params\": best_gen_kwargs,\n",
    "    \"decoder_sweep_wer\": round(optimized_wer, 4),\n",
    "    \"decoder_improvement\": round(decoder_improvement, 4),\n",
    "    \"decoder_improvement_percent\": round(decoder_improvement_pct, 2),\n",
    "    \"decoder_sweep_df\": decoder_df,\n",
    "    \"optimized_results\": optimized_results\n",
    "}\n",
    "\n",
    "print(\"\\nüéâ Decoder sweep complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03044a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n==================================================\n",
      "üíæ SAVING RESULTS\n",
      "==================================================\n",
      "üìÅ Created directory: ./experiments/finetuning-34\n",
      "üìÑ Saved predictions: ./experiments/finetuning-34/finetuning-34_predictions.csv\n",
      "üìÑ Saved optimized predictions: ./experiments/finetuning-34/finetuning-34_predictions_optimized.csv\n",
      "üìÑ Saved decoder sweep results: ./experiments/finetuning-34/decoder_sweep_results.csv\n",
      "üìÑ Saved summary: ./experiments/finetuning-34/finetuning-34_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SAVE RESULTS\n",
    "# ================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"üíæ SAVING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "overall_end_time = time.time()\n",
    "overall_duration_secs = int(overall_end_time - overall_start_time)\n",
    "overall_duration_hms = str(datetime.timedelta(seconds=overall_duration_secs))\n",
    "\n",
    "# Create output directory\n",
    "CSV_OUTPUT_DIR = f\"./experiments/{EXPERIMENT_NAME}\"\n",
    "os.makedirs(CSV_OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Created directory: {CSV_OUTPUT_DIR}\")\n",
    "\n",
    "# Save original post-training predictions\n",
    "samplewise_data = []\n",
    "for i in range(len(post_results[\"predictions\"])):\n",
    "    samplewise_data.append({\n",
    "        \"reference_raw\": post_results[\"references_raw\"][i],\n",
    "        \"reference_normalized\": post_results[\"references\"][i],\n",
    "        \"prediction_raw\": post_results[\"predictions_raw\"][i],\n",
    "        \"prediction_normalized\": post_results[\"predictions\"][i],\n",
    "        \"wer\": round(post_results[\"sample_wers\"][i], 4)\n",
    "    })\n",
    "\n",
    "NEW_PREDICTIONS_CSV = f\"{CSV_OUTPUT_DIR}/{PREDICTIONS_CSV}\"\n",
    "pd.DataFrame(samplewise_data).to_csv(NEW_PREDICTIONS_CSV, index=False)\n",
    "print(f\"üìÑ Saved predictions: {NEW_PREDICTIONS_CSV}\")\n",
    "\n",
    "# Save optimized predictions (from decoder sweep)\n",
    "if 'decoder_sweep_data' in locals():\n",
    "    optimized_predictions_csv = f\"{CSV_OUTPUT_DIR}/{EXPERIMENT_NAME}_predictions_optimized.csv\"\n",
    "    optimized_data = [{\n",
    "        \"reference_raw\": decoder_sweep_data[\"optimized_results\"][\"references_raw\"][i],\n",
    "        \"reference_normalized\": decoder_sweep_data[\"optimized_results\"][\"references\"][i],\n",
    "        \"prediction_raw\": decoder_sweep_data[\"optimized_results\"][\"predictions_raw\"][i],\n",
    "        \"prediction_normalized\": decoder_sweep_data[\"optimized_results\"][\"predictions\"][i],\n",
    "        \"wer\": round(decoder_sweep_data[\"optimized_results\"][\"sample_wers\"][i], 4)\n",
    "    } for i in range(len(decoder_sweep_data[\"optimized_results\"][\"predictions\"]))]\n",
    "    \n",
    "    pd.DataFrame(optimized_data).to_csv(optimized_predictions_csv, index=False)\n",
    "    print(f\"üìÑ Saved optimized predictions: {optimized_predictions_csv}\")\n",
    "    \n",
    "    # Save decoder sweep results\n",
    "    SWEEP_CSV = f\"{CSV_OUTPUT_DIR}/decoder_sweep_results.csv\"\n",
    "    decoder_sweep_data[\"decoder_sweep_df\"].to_csv(SWEEP_CSV, index=False)\n",
    "    print(f\"üìÑ Saved decoder sweep results: {SWEEP_CSV}\")\n",
    "\n",
    "# Save run summary\n",
    "summary_data = {\n",
    "    \"experiment_name\": EXPERIMENT_NAME,\n",
    "    \"base_model\": BASE_MODEL_NAME,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"lora_dropout\": LORA_DROPOUT,\n",
    "    \"target_modules\": str(LORA_TARGET_MODULES),\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_train_epochs\": NUM_EPOCHS,\n",
    "    \"train_1\": train_1 if train_1 is not None else \"-\",\n",
    "    \"train_2\": train_2 if train_2 is not None else \"-\",\n",
    "    \"train_3\": train_3 if train_3 is not None else \"-\",\n",
    "    \"test_1\":  test_1 if test_1 is not None else \"-\",\n",
    "    \"test_2\":  test_2 if test_2 is not None else \"-\",\n",
    "    \"test_3\":  test_3 if test_3 is not None else \"-\",\n",
    "    \"train_num_samples_cap\": TRAIN_NUM_SAMPLES if TRAIN_NUM_SAMPLES else \"full\",\n",
    "    \"test_num_samples_cap\": TEST_NUM_SAMPLES if TEST_NUM_SAMPLES else \"full\",\n",
    "    \"eval_from_train_pct\": EVAL_FROM_TRAIN_PCT,\n",
    "    \"train_set_size\": len(train_ds),\n",
    "    \"validation_set_size\": len(validation_ds) if validation_ds else 0,\n",
    "    \"test_set_size\": len(test_ds),\n",
    "    \"total_start_time\": datetime.datetime.fromtimestamp(overall_start_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"total_end_time\": datetime.datetime.fromtimestamp(overall_end_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"total_duration\": overall_duration_hms,\n",
    "    \"train_start_time\": datetime.datetime.fromtimestamp(train_start_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"train_end_time\": datetime.datetime.fromtimestamp(train_end_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"train_duration\": train_duration_hms,\n",
    "    \"fp16_enabled\": FP16,\n",
    "    \"pre_training_wer\": round(pre_training_wer, 4),\n",
    "    \"post_training_wer\": round(post_training_wer, 4),\n",
    "    \"wer_improvement\": round(wer_improvement, 4),\n",
    "    \"wer_improvement_percent\": round(wer_improvement_pct, 2) if BASE_MODEL_NAME == \"openai/whisper-large-v2-turbo\" or BASE_MODEL_NAME == \"openai/whisper-large-v3-turbo\" else '-',\n",
    "    \"wer_improvement_large\": round(wer_improvement_pct, 2) if BASE_MODEL_NAME == \"openai/whisper-large-v2\" or BASE_MODEL_NAME == \"openai/whisper-large-v3\" else '-'\n",
    "}\n",
    "\n",
    "# Add validation WERs\n",
    "for i, wer in enumerate(validation_wers, start=1):\n",
    "    summary_data[f\"validation_{i}\"] = wer\n",
    "\n",
    "# Add decoder sweep data if available\n",
    "if 'decoder_sweep_data' in locals():\n",
    "    summary_data.update({\n",
    "        \"best_decoder_config\": decoder_sweep_data[\"best_decoder_config\"],\n",
    "        \"best_decoder_params\": str(decoder_sweep_data[\"best_decoder_config_params\"]),\n",
    "        \"decoder_optimized_wer\": decoder_sweep_data[\"decoder_sweep_wer\"],\n",
    "        \"decoder_improvement\": decoder_sweep_data[\"decoder_improvement\"],\n",
    "        \"decoder_improvement_percent\": decoder_sweep_data[\"decoder_improvement_percent\"]\n",
    "    })\n",
    "\n",
    "NEW_SUMMARY_CSV = f\"{CSV_OUTPUT_DIR}/{SUMMARY_CSV}\"\n",
    "pd.DataFrame([summary_data]).to_csv(NEW_SUMMARY_CSV, index=False)\n",
    "print(f\"üìÑ Saved summary: {NEW_SUMMARY_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n==================================================\n",
      "üéâ EXPERIMENT COMPLETE\n",
      "==================================================\n",
      "Total duration: 16:41:15\n",
      "\\nüìä Results:\n",
      "   Pre-training WER:  0.2283 (22.83%)\n",
      "   Post-training WER: 0.2287 (22.87%)\n",
      "   Fine-tuning Improvement: -0.0004 (-0.18%)\n",
      "\\nüî¨ Decoder Optimization:\n",
      "   Best Config: cfg_46\n",
      "   Optimized WER: 0.2187 (21.87%)\n",
      "   Decoder Improvement: 0.0100 (4.35%)\n",
      "\\nüöÄ TOTAL IMPROVEMENT (Fine-tuning + Decoder):\n",
      "   Baseline WER: 0.2283\n",
      "   Final WER: 0.2187\n",
      "   Total Improvement: 0.0096 (4.19%)\n",
      "\\nüìÅ Output files:\n",
      "   - finetuning-34_predictions.csv\n",
      "   - finetuning-34_predictions_optimized.csv\n",
      "   - decoder_sweep_results.csv\n",
      "   - finetuning-34_summary.csv\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# FINAL SUMMARY\n",
    "# ================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"üéâ EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total duration: {overall_duration_hms}\")\n",
    "print(f\"\\\\nüìä Results:\")\n",
    "print(f\"   Pre-training WER:  {pre_training_wer:.4f} ({pre_training_wer*100:.2f}%)\")\n",
    "print(f\"   Post-training WER: {post_training_wer:.4f} ({post_training_wer*100:.2f}%)\")\n",
    "print(f\"   Fine-tuning Improvement: {wer_improvement:.4f} ({wer_improvement_pct:.2f}%)\")\n",
    "\n",
    "# Add decoder sweep results if available\n",
    "if 'decoder_sweep_data' in locals():\n",
    "    print(f\"\\\\nüî¨ Decoder Optimization:\")\n",
    "    print(f\"   Best Config: {decoder_sweep_data['best_decoder_config']}\")\n",
    "    print(f\"   Optimized WER: {decoder_sweep_data['decoder_sweep_wer']:.4f} ({decoder_sweep_data['decoder_sweep_wer']*100:.2f}%)\")\n",
    "    print(f\"   Decoder Improvement: {decoder_sweep_data['decoder_improvement']:.4f} ({decoder_sweep_data['decoder_improvement_percent']:.2f}%)\")\n",
    "    \n",
    "    # Total improvement from baseline\n",
    "    total_improvement = pre_training_wer - decoder_sweep_data['decoder_sweep_wer']\n",
    "    total_improvement_pct = (total_improvement / pre_training_wer) * 100\n",
    "    print(f\"\\\\nüöÄ TOTAL IMPROVEMENT (Fine-tuning + Decoder):\")\n",
    "    print(f\"   Baseline WER: {pre_training_wer:.4f}\")\n",
    "    print(f\"   Final WER: {decoder_sweep_data['decoder_sweep_wer']:.4f}\")\n",
    "    print(f\"   Total Improvement: {total_improvement:.4f} ({total_improvement_pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\\\nüìÅ Output files:\")\n",
    "print(f\"   - {PREDICTIONS_CSV}\")\n",
    "if 'decoder_sweep_data' in locals():\n",
    "    print(f\"   - {EXPERIMENT_NAME}_predictions_optimized.csv\")\n",
    "    print(f\"   - decoder_sweep_results.csv\")\n",
    "print(f\"   - {SUMMARY_CSV}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695044b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n==================================================\n",
      "üíæ SAVING FINE-TUNED MODEL\n",
      "==================================================\n",
      "üìÅ Created directory: ./saved_models/finetuning-34/lora_adapter\n",
      "\\nüîß Saving LoRA adapter weights...\n",
      "‚úÖ LoRA adapter saved to: ./saved_models/finetuning-34/lora_adapter\n",
      "‚úÖ Best generation config saved to: ./saved_models/finetuning-34/lora_adapter/best_generation_config.json\n",
      "‚úÖ Training config saved to: ./saved_models/finetuning-34/lora_adapter/training_config.json\n",
      "\\n==================================================\n",
      "üéâ MODEL SAVING COMPLETE\n",
      "==================================================\n",
      "\\nüì¶ Saved files:\n",
      "   LoRA Adapter: ./saved_models/finetuning-34/lora_adapter\n",
      "   - adapter_model.safetensors (LoRA weights)\n",
      "   - adapter_config.json (LoRA configuration)\n",
      "   - preprocessor_config.json & tokenizer files\n",
      "   - training_config.json (your training settings)\n",
      "   - best_generation_config.json (optimized decoder params)\n",
      "\\nüìù To load the model later, use:\n",
      "\n",
      "\n",
      "# Load base model\n",
      "base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3-turbo\")\n",
      "\n",
      "# Load LoRA adapter\n",
      "model = PeftModel.from_pretrained(base_model, \"./saved_models/finetuning-34/lora_adapter\")\n",
      "\n",
      "# Load processor\n",
      "processor = WhisperProcessor.from_pretrained(\"./saved_models/finetuning-34/lora_adapter\")\n",
      "\n",
      "\n",
      "# Load best generation config\n",
      "with open(\"./saved_models/finetuning-34/lora_adapter/best_generation_config.json\", \"r\") as f:\n",
      "    best_gen_config = json.load(f)\n",
      "\n",
      "# Use for inference\n",
      "pred_ids = model.generate(input_features=input_features, **best_gen_config)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SAVE FINE-TUNED MODEL\n",
    "# ================================\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"üíæ SAVING FINE-TUNED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define output directory\n",
    "OUTPUT_DIR = f\"./saved_models/{EXPERIMENT_NAME}\"\n",
    "LORA_ADAPTER_DIR = f\"{OUTPUT_DIR}/lora_adapter\"\n",
    "MERGED_MODEL_DIR = f\"{OUTPUT_DIR}/merged_model\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(LORA_ADAPTER_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Created directory: {LORA_ADAPTER_DIR}\")\n",
    "\n",
    "# 1. Save LoRA adapter weights (lightweight, recommended)\n",
    "print(\"\\\\nüîß Saving LoRA adapter weights...\")\n",
    "model.save_pretrained(LORA_ADAPTER_DIR)\n",
    "processor.save_pretrained(LORA_ADAPTER_DIR)\n",
    "print(f\"‚úÖ LoRA adapter saved to: {LORA_ADAPTER_DIR}\")\n",
    "\n",
    "# 2. Save configuration info\n",
    "config_info = {\n",
    "    \"base_model\": BASE_MODEL_NAME,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"lora_dropout\": LORA_DROPOUT,\n",
    "    \"target_modules\": LORA_TARGET_MODULES,\n",
    "    \"training_epochs\": NUM_EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"train_datasets\": [train_1, train_2, train_3],\n",
    "    \"test_datasets\": [test_1, test_2, test_3],\n",
    "    \"train_samples\": len(train_ds),\n",
    "    \"test_samples\": len(test_ds),\n",
    "    \"pre_training_wer\": round(pre_training_wer, 4),\n",
    "    \"post_training_wer\": round(post_training_wer, 4),\n",
    "    \"wer_improvement\": round(wer_improvement, 4)\n",
    "}\n",
    "\n",
    "# Add decoder sweep info if available\n",
    "if 'decoder_sweep_data' in locals():\n",
    "    # Convert numpy/pandas types to native Python types for JSON serialization\n",
    "    best_decoder_params_clean = {}\n",
    "    for k, v in decoder_sweep_data[\"best_decoder_config_params\"].items():\n",
    "        if isinstance(v, (np.integer, np.int64)):\n",
    "            best_decoder_params_clean[k] = int(v)\n",
    "        elif isinstance(v, (np.floating, np.float64)):\n",
    "            best_decoder_params_clean[k] = float(v)\n",
    "        elif isinstance(v, np.bool_):\n",
    "            best_decoder_params_clean[k] = bool(v)\n",
    "        else:\n",
    "            best_decoder_params_clean[k] = v\n",
    "    \n",
    "    config_info.update({\n",
    "        \"decoder_optimization\": True,\n",
    "        \"best_decoder_config\": decoder_sweep_data[\"best_decoder_config\"],\n",
    "        \"best_decoder_params\": best_decoder_params_clean,  # Use cleaned version\n",
    "        \"optimized_wer\": decoder_sweep_data[\"decoder_sweep_wer\"],\n",
    "        \"total_wer_improvement\": round(pre_training_wer - decoder_sweep_data[\"decoder_sweep_wer\"], 4)\n",
    "    })\n",
    "    \n",
    "    # Save best generation config for inference (with cleaned types)\n",
    "    generation_config_path = f\"{LORA_ADAPTER_DIR}/best_generation_config.json\"\n",
    "    with open(generation_config_path, \"w\") as f:\n",
    "        json.dump(best_decoder_params_clean, f, indent=2)  # Use cleaned version\n",
    "    print(f\"‚úÖ Best generation config saved to: {generation_config_path}\")\n",
    "else:\n",
    "    config_info[\"decoder_optimization\"] = False\n",
    "\n",
    "with open(f\"{LORA_ADAPTER_DIR}/training_config.json\", \"w\") as f:\n",
    "    json.dump(config_info, f, indent=2)\n",
    "print(f\"‚úÖ Training config saved to: {LORA_ADAPTER_DIR}/training_config.json\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"üéâ MODEL SAVING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\\\nüì¶ Saved files:\")\n",
    "print(f\"   LoRA Adapter: {LORA_ADAPTER_DIR}\")\n",
    "print(f\"   - adapter_model.safetensors (LoRA weights)\")\n",
    "print(f\"   - adapter_config.json (LoRA configuration)\")\n",
    "print(f\"   - preprocessor_config.json & tokenizer files\")\n",
    "print(f\"   - training_config.json (your training settings)\")\n",
    "if 'decoder_sweep_data' in locals():\n",
    "    print(f\"   - best_generation_config.json (optimized decoder params)\")\n",
    "\n",
    "print(f\"\\\\nüìù To load the model later, use:\")\n",
    "print(f\"\"\"\n",
    "\n",
    "# Load base model\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\"{BASE_MODEL_NAME}\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"{LORA_ADAPTER_DIR}\")\n",
    "\n",
    "# Load processor\n",
    "processor = WhisperProcessor.from_pretrained(\"{LORA_ADAPTER_DIR}\")\n",
    "\"\"\")\n",
    "\n",
    "if 'decoder_sweep_data' in locals():\n",
    "    print(f\"\"\"\n",
    "# Load best generation config\n",
    "with open(\"{LORA_ADAPTER_DIR}/best_generation_config.json\", \"r\") as f:\n",
    "    best_gen_config = json.load(f)\n",
    "\n",
    "# Use for inference\n",
    "pred_ids = model.generate(input_features=input_features, **best_gen_config)\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
