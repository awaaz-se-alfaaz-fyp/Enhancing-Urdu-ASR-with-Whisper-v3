{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81de26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaider/miniconda3/envs/whisper-lora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/shaider/miniconda3/envs/whisper-lora/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from jiwer import wer\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths (relative to the script's directory)\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "input_folder = os.path.join(script_dir, \"Whisper turbo FT\")  # Folder with input text files (transcriptions)\n",
    "output_folder = os.path.join(script_dir, \"output_transcripts\")  # Folder to save model outputs (create if needed)\n",
    "gold_folder = os.path.join(script_dir, \"Gold Transcriptions\")  # Folder with gold transcriptions\n",
    "mapping_csv = os.path.join(script_dir, \"mapping.csv\")  # CSV with 'Name' and 'Gold_path' columns\n",
    "csv_path = os.path.join(script_dir, \"results.csv\")  # Path to save the results CSV\n",
    "os.makedirs(output_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16e1e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Model details\n",
    "model_name = \"Qwen/Qwen3-14B\"\n",
    "HF_TOKEN = \"HF_TOKEN\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",  # Automatically places model on GPU if available\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "print(\"Model loaded on:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e7b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an Urdu ASR error correction expert. Your ONLY task is to replace incorrectly transcribed Urdu words with their correct forms.\n",
    "CRITICAL RULES:\n",
    "- ABSOLUTELY NO punctuation (no ۔ ، ؟ ! . , : ; \" ' or any symbols)\n",
    "- NO new words or phrases\n",
    "- NO reordering\n",
    "- NO grammar changes\n",
    "- ONLY replace wrong words with correct ones\n",
    "- If unsure about a word, leave it unchanged\n",
    "Think of this as a word-by-word dictionary replacement, not a rewrite.\n",
    "\n",
    "Fix ONLY the incorrectly transcribed words in this Urdu text. Replace wrong words with correct ones based on context. Add NO punctuation.\n",
    "\n",
    "Urdu text: {text}\n",
    "\n",
    "Corrected text:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3Alleged Gold.txt\n",
      "Processed 11Deaths Gold.txt\n",
      "Processed AfghanCricketGPT.txt\n",
      "Processed BuildingCollapse Gold.txt\n",
      "Processed BullyingGPT Gold.txt\n",
      "Processed ConstructionHalt_Gold.txt\n",
      "Processed CTD_Gold.txt\n",
      "Processed GasTheft_Gold.txt\n",
      "Processed Hamid Mir Imran Khan Key.txt\n",
      "Processed Inflation Gold.txt\n",
      "Processed KarachiKings Gold.txt\n",
      "Processed Kidney Gold.txt\n",
      "Processed MobileTheft Gold.txt\n",
      "Processed Murree Gold.txt\n",
      "Processed PakvsInd2 Gold.txt\n",
      "Processed PakVsInd Gold.txt\n",
      "Processed PassengerGPT.txt\n",
      "Processed Petrol Gold.txt\n",
      "Processed PunjabGovt Gold.txt\n",
      "Processed Quetta Gold.txt\n",
      "Processed RamadanGas Gold.txt\n",
      "Processed RamadanMoon Gold.txt\n",
      "Processed RedLine_Gold.txt\n",
      "Processed Sama Electricity Relief Key.txt\n",
      "Processed Sama FM Egypt Visit Key.txt\n",
      "Processed Sama PSL Multan Key.txt\n",
      "Processed SindhTax_Gold.txt\n",
      "Processed Traffic_Accident_Geo.txt\n",
      "Processed VehicleCollisionGPT.txt\n",
      "Processed Women Gold.txt\n"
     ]
    }
   ],
   "source": [
    "# Read the mapping CSV\n",
    "with open(mapping_csv, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    mappings = list(reader)\n",
    "\n",
    "# Process each file based on mapping\n",
    "for mapping in mappings:\n",
    "    filename = mapping['Name']\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        print(f\"Skipping non-txt file: {filename}\")\n",
    "        continue\n",
    "    \n",
    "    input_path = os.path.join(input_folder, filename)\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Input file not found: {input_path}\")\n",
    "        continue\n",
    "    \n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        input_text = f.read()\n",
    "    \n",
    "    # Format the prompt with the input text\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_template.format(text=input_text)}]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True,\n",
    "    )\n",
    "\n",
    "    model_inputs = tokenizer([input_ids], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with beam search for more deterministic and accurate corrections\n",
    "    # These settings promote minimal, conservative changes: no sampling, beam search for quality\n",
    "    outputs = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768,  # Adjust to roughly match input length + margin if needed\n",
    "    )\n",
    "    output_ids = outputs[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    generated_text = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    #generateed text is the actual content\n",
    "    \n",
    "    # Save to output folder with the same filename\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(generated_text.strip())\n",
    "    \n",
    "    print(f\"Processed {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08de1f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER for 3Alleged Gold.txt (before SLM): 0.22145328719723184\n",
      "WER for 3Alleged Gold.txt (after SLM): 0.2179930795847751\n",
      "WER for 11Deaths Gold.txt (before SLM): 0.282798833819242\n",
      "WER for 11Deaths Gold.txt (after SLM): 0.24198250728862974\n",
      "WER for AfghanCricketGPT.txt (before SLM): 0.23371647509578544\n",
      "WER for AfghanCricketGPT.txt (after SLM): 0.19157088122605365\n",
      "WER for BuildingCollapse Gold.txt (before SLM): 0.20892018779342722\n",
      "WER for BuildingCollapse Gold.txt (after SLM): 0.19953051643192488\n",
      "WER for BullyingGPT Gold.txt (before SLM): 0.2542372881355932\n",
      "WER for BullyingGPT Gold.txt (after SLM): 0.2245762711864407\n",
      "WER for ConstructionHalt_Gold.txt (before SLM): 0.20085470085470086\n",
      "WER for ConstructionHalt_Gold.txt (after SLM): 0.20085470085470086\n",
      "WER for CTD_Gold.txt (before SLM): 0.33796296296296297\n",
      "WER for CTD_Gold.txt (after SLM): 0.33796296296296297\n",
      "WER for GasTheft_Gold.txt (before SLM): 0.2364217252396166\n",
      "WER for GasTheft_Gold.txt (after SLM): 0.22044728434504793\n",
      "WER for Hamid Mir Imran Khan Key.txt (before SLM): 0.11290322580645161\n",
      "WER for Hamid Mir Imran Khan Key.txt (after SLM): 0.11290322580645161\n",
      "WER for Inflation Gold.txt (before SLM): 0.2116788321167883\n",
      "WER for Inflation Gold.txt (after SLM): 0.18613138686131386\n",
      "WER for KarachiKings Gold.txt (before SLM): 0.20707070707070707\n",
      "WER for KarachiKings Gold.txt (after SLM): 0.16666666666666666\n",
      "WER for Kidney Gold.txt (before SLM): 0.3850746268656716\n",
      "WER for Kidney Gold.txt (after SLM): 0.3761194029850746\n",
      "WER for MobileTheft Gold.txt (before SLM): 0.11377245508982035\n",
      "WER for MobileTheft Gold.txt (after SLM): 0.11077844311377245\n",
      "WER for Murree Gold.txt (before SLM): 0.3271276595744681\n",
      "WER for Murree Gold.txt (after SLM): 0.31648936170212766\n",
      "WER for PakvsInd2 Gold.txt (before SLM): 0.22672064777327935\n",
      "WER for PakvsInd2 Gold.txt (after SLM): 0.21862348178137653\n",
      "WER for PakVsInd Gold.txt (before SLM): 0.1564625850340136\n",
      "WER for PakVsInd Gold.txt (after SLM): 0.1564625850340136\n",
      "WER for PassengerGPT.txt (before SLM): 0.2925170068027211\n",
      "WER for PassengerGPT.txt (after SLM): 0.2789115646258503\n",
      "WER for Petrol Gold.txt (before SLM): 0.7136150234741784\n",
      "WER for Petrol Gold.txt (after SLM): 0.6713615023474179\n",
      "WER for PunjabGovt Gold.txt (before SLM): 0.2831858407079646\n",
      "WER for PunjabGovt Gold.txt (after SLM): 0.28761061946902655\n",
      "WER for Quetta Gold.txt (before SLM): 0.48\n",
      "WER for Quetta Gold.txt (after SLM): 0.48\n",
      "WER for RamadanGas Gold.txt (before SLM): 0.1592920353982301\n",
      "WER for RamadanGas Gold.txt (after SLM): 0.1415929203539823\n",
      "WER for RamadanMoon Gold.txt (before SLM): 0.2565597667638484\n",
      "WER for RamadanMoon Gold.txt (after SLM): 0.24489795918367346\n",
      "WER for RedLine_Gold.txt (before SLM): 0.2302839116719243\n",
      "WER for RedLine_Gold.txt (after SLM): 0.2302839116719243\n",
      "WER for Sama Electricity Relief Key.txt (before SLM): 0.18340611353711792\n",
      "WER for Sama Electricity Relief Key.txt (after SLM): 0.1703056768558952\n",
      "WER for Sama FM Egypt Visit Key.txt (before SLM): 0.17647058823529413\n",
      "WER for Sama FM Egypt Visit Key.txt (after SLM): 0.1638655462184874\n",
      "WER for Sama PSL Multan Key.txt (before SLM): 0.10815939278937381\n",
      "WER for Sama PSL Multan Key.txt (after SLM): 0.10815939278937381\n",
      "WER for SindhTax_Gold.txt (before SLM): 0.28761061946902655\n",
      "WER for SindhTax_Gold.txt (after SLM): 0.2743362831858407\n",
      "WER for Traffic_Accident_Geo.txt (before SLM): 0.27697841726618705\n",
      "WER for Traffic_Accident_Geo.txt (after SLM): 0.26618705035971224\n",
      "WER for VehicleCollisionGPT.txt (before SLM): 0.3004484304932735\n",
      "WER for VehicleCollisionGPT.txt (after SLM): 0.273542600896861\n",
      "WER for Women Gold.txt (before SLM): 0.11764705882352941\n",
      "WER for Women Gold.txt (after SLM): 0.13003095975232198\n",
      "Average WER before SLM: 0.25277834686208095\n",
      "Average WER after SLM: 0.24000595818472334\n"
     ]
    }
   ],
   "source": [
    "# Now compute WER for each pair (before and after SLM) based on mapping\n",
    "results = []\n",
    "for mapping in mappings:\n",
    "    filename = mapping['Name']\n",
    "    gold_filename = mapping['Gold_path']\n",
    "    if not filename.endswith(\".txt\") or not gold_filename.endswith(\".txt\"):\n",
    "        print(f\"Skipping non-txt pair: {filename} / {gold_filename}\")\n",
    "        continue\n",
    "    \n",
    "    input_path = os.path.join(input_folder, filename)\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    gold_path = os.path.join(gold_folder, gold_filename)\n",
    "    \n",
    "    if os.path.exists(gold_path) and os.path.exists(input_path) and os.path.exists(output_path):\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            input_text = f.read().strip()\n",
    "        with open(output_path, 'r', encoding='utf-8') as f:\n",
    "            output_text = f.read().strip()\n",
    "        with open(gold_path, 'r', encoding='utf-8') as f:\n",
    "            gold_text = f.read().strip()\n",
    "        \n",
    "        wer_before = wer(gold_text, input_text)  # WER between gold and input (before SLM)\n",
    "        wer_after = wer(gold_text, output_text)   # WER between gold and output (after SLM)\n",
    "        \n",
    "        results.append({\n",
    "            'filename': filename,\n",
    "            'gold_filename': gold_filename,\n",
    "            'wer_before': wer_before,\n",
    "            'wer_after': wer_after\n",
    "        })\n",
    "        \n",
    "        print(f\"WER for {filename} (before SLM): {wer_before}\")\n",
    "        print(f\"WER for {filename} (after SLM): {wer_after}\")\n",
    "    else:\n",
    "        print(f\"Files not found for pair: {filename} / {gold_filename}\")\n",
    "\n",
    "# Write to CSV\n",
    "if results:\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['filename', 'gold_filename', 'wer_before', 'wer_after']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "    \n",
    "    # Average WER\n",
    "    avg_wer_before = sum(r['wer_before'] for r in results) / len(results)\n",
    "    avg_wer_after = sum(r['wer_after'] for r in results) / len(results)\n",
    "    print(f\"Average WER before SLM: {avg_wer_before}\")\n",
    "    print(f\"Average WER after SLM: {avg_wer_after}\")\n",
    "else:\n",
    "    print(\"No WER calculations performed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
