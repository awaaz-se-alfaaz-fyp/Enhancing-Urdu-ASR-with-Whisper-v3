{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597c63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashaikh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# IMPORTS\n",
    "# ================================\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset, Dataset, Audio, DatasetDict, concatenate_datasets\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import AdamW\n",
    "import evaluate\n",
    "from jiwer import wer as jiwer_wer\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcb618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# CONFIGURATION\n",
    "# ================================\n",
    "\n",
    "# Experiment settings\n",
    "EXPERIMENT_NAME = \"finetuning-6\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Model and LoRA config\n",
    "BASE_MODEL_NAME = \"openai/whisper-large-v3-turbo\" # \"openai/whisper-large-v3-turbo\" # \"openai/whisper-large-v3\"\n",
    "LORA_R = 16 #32\n",
    "LORA_ALPHA = 32 #64\n",
    "LORA_DROPOUT = 0.05\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\"] #, \"k_proj\", \"out_proj\"]\n",
    "\n",
    "# Training config\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 2 #12\n",
    "FP16 = True\n",
    "MAX_LABEL_LENGTH = 128\n",
    "\n",
    "# Dataset config\n",
    "TARGET_SR = 16000\n",
    "AUDIO_COL = \"audio\"\n",
    "TEXT_COL = \"transcription\"\n",
    "TRAIN_NUM_SAMPLES = 8000  # None = full set\n",
    "TEST_NUM_SAMPLES = None   # None = full set\n",
    "EVAL_FROM_TRAIN_PCT = 0.0  # 0.05 = 5% validation from train\n",
    "\n",
    "# Output files\n",
    "PREDICTIONS_CSV = f\"{EXPERIMENT_NAME}_predictions.csv\"\n",
    "SUMMARY_CSV = f\"{EXPERIMENT_NAME}_summary.csv\"\n",
    "\n",
    "# Set random seeds\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633971f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick from: \"commonvoice\", \"fleurs\", \"csalt\" or None\n",
    "# At minimum, train_1 and test_1 must be non-None\n",
    "train_1 = \"commonvoice\"\n",
    "train_2 = \"fleurs\"\n",
    "train_3 = None\n",
    "\n",
    "test_1  = \"csalt\"\n",
    "test_2  = None\n",
    "test_3  = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a64f55",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ba5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Text normalization function loaded\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# TEXT NORMALIZATION\n",
    "# ================================\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Callable\n",
    "from itertools import product\n",
    "\n",
    "# -----------------------------\n",
    "# Core normalization utilities\n",
    "# -----------------------------\n",
    "\n",
    "_ARABIC_DIACRITICS = re.compile(\n",
    "    \"[\"                             # Arabic diacritics range\n",
    "    \"\\u0610-\\u061A\"                 # honorifics, small high\n",
    "    \"\\u064B-\\u065F\"                 # tanwin/harakat\n",
    "    \"\\u0670\"                        # superscript alef\n",
    "    \"\\u06D6-\\u06ED\"                 # Quranic marks\n",
    "    \"]\"\n",
    ")\n",
    "\n",
    "# Zero-width & elongation\n",
    "_ZW_CHARS = re.compile(\"[\\u200B-\\u200F\\u202A-\\u202E\\u2066-\\u2069]\")\n",
    "_KASHIDA  = re.compile(\"\\u0640\")  # tatweel\n",
    "\n",
    "# Arabic presentation forms (NFKC will canonicalize most)\n",
    "def _compat_normalize(s: str) -> str:\n",
    "    # Normalize compatibility forms and spacing\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    # Remove bidi/zero-width and kashida\n",
    "    s = _ZW_CHARS.sub(\"\", s)\n",
    "    s = _KASHIDA.sub(\"\", s)\n",
    "    # Remove diacritics\n",
    "    s = _ARABIC_DIACRITICS.sub(\"\", s)\n",
    "    return s\n",
    "\n",
    "# Map Arabic/Urdu codepoints to a single canonical set often used in Urdu\n",
    "# (Farsi Yeh, Heh goal, etc.)\n",
    "def _canonical_codepoints(s: str) -> str:\n",
    "    # Unify Yeh forms: U+064A (Arabic Yeh), U+06CC (Farsi Yeh) -> choose U+06CC\n",
    "    s = s.replace(\"\\u064A\", \"\\u06CC\")\n",
    "    # Unify Alef Maksura (rare in Urdu) to Farsi Yeh as well (defensive)\n",
    "    s = s.replace(\"\\u0649\", \"\\u06CC\")\n",
    "    # Unify Heh goal variants: ÿ©/Ÿá/€Å/€Ç ‚Üí €Å (U+06C1) when appropriate\n",
    "    # Keep it simple/robust for scoring:\n",
    "    s = s.replace(\"\\u06C0\", \"\\u06C1\")  # heh with hamza above ‚Üí heh goal\n",
    "    # Don't over-aggressively rewrite 'Ÿá' to '€Å' (Arabic heh to Urdu heh goal),\n",
    "    # but we can do a light pass:\n",
    "    s = re.sub(r\"(?<=\\S)\\u0647(?=\\b)\", \"\\u06C1\", s)  # word-final Arabic heh ‚Üí Urdu heh goal\n",
    "    return s\n",
    "\n",
    "# Digits: normalize both Latin and Arabic-Indic to Arabic-Indic (or remove)\n",
    "_ARABIC_INDIC_DIGITS = str.maketrans(\n",
    "    \"0123456789\"\n",
    "    \"Ÿ†Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©\"\n",
    "    \"€∞€±€≤€≥€¥€µ€∂€∑€∏€π\",\n",
    "    \"€∞€±€≤€≥€¥€µ€∂€∑€∏€π\" * 3  # map Latin + Arabic-Indic + Extended to Extended Arabic-Indic\n",
    ")\n",
    "def _normalize_digits(s: str) -> str:\n",
    "    return s.translate(_ARABIC_INDIC_DIGITS)\n",
    "\n",
    "# Remove punctuation & special markers (keep intra-word apostrophes if you want)\n",
    "_PUNCT = re.compile(r\"[^\\w\\s\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF]\")  # drop non-Arabic/word chars\n",
    "# Seamless-style disfluencies: remove tokens like #um #uh #laugh\n",
    "_SEAMLESS_DISFL = re.compile(r\"(?<!\\w)#\\w+\")\n",
    "\n",
    "def _strip_punct_and_disfluencies(s: str) -> str:\n",
    "    s = _SEAMLESS_DISFL.sub(\" \", s)\n",
    "    # Convert underscores/odd joins to space first (defensive)\n",
    "    s = s.replace(\"_\", \" \")\n",
    "    s = _PUNCT.sub(\" \", s)\n",
    "    return s\n",
    "\n",
    "def _squash_spaces(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "\n",
    "# -------------------------------------------\n",
    "# Orthographic + token-segmentation variants\n",
    "# -------------------------------------------\n",
    "\n",
    "# Frequent variants noted in paper: \"⁄Üÿß€Å€å€í\" spellings; \"€ÅŸà ⁄Øÿß/€ÅŸà⁄Øÿß\" etc.\n",
    "_VARIANT_CANON = [\n",
    "    # --- ⁄Üÿß€Å€å€í (imperative/necessity) canonicalization ---\n",
    "    # Variants: ⁄Üÿß€Å€åÿ¶€í / ⁄Üÿß⁄æ€å€í / ⁄Üÿß€Åÿ¶€í / ⁄Üÿß€Å€å€ì, etc ‚Üí ⁄Üÿß€Å€å€í\n",
    "    (re.compile(r\"\\b⁄Üÿß€Å€å[ÿ¶€í€ì]\\b\"), \"⁄Üÿß€Å€å€í\"),\n",
    "    (re.compile(r\"\\b⁄Üÿß⁄æ€å[ÿ¶€í€ì]\\b\"), \"⁄Üÿß€Å€å€í\"),\n",
    "    (re.compile(r\"\\b⁄Üÿß€Å[ÿ¶€í€ì]\\b\"), \"⁄Üÿß€Å€å€í\"),\n",
    "    # common stem \"chahie\" unvoweled variants\n",
    "    (re.compile(r\"\\b⁄Üÿß€Å€å?€í\\b\"), \"⁄Üÿß€Å€å€í\"),\n",
    "\n",
    "    # --- €ÅŸà⁄Øÿß family: space-insensitive joining ---\n",
    "    (re.compile(r\"\\b€ÅŸà\\s+⁄Øÿß\\b\"), \"€ÅŸà⁄Øÿß\"),\n",
    "    (re.compile(r\"\\b€ÅŸà\\s+⁄Ø€å\\b\"), \"€ÅŸà⁄Ø€å\"),\n",
    "    (re.compile(r\"\\b€ÅŸà\\s+⁄Ø€í\\b\"), \"€ÅŸà⁄Ø€í\"),\n",
    "    # The reverse (split) hardly needed if we canonicalize to joined forms\n",
    "\n",
    "    # Misc. common merges/splits seen in practice (add as you observe)\n",
    "    (re.compile(r\"\\b⁄©Ÿà ÿ¶€å\\b\"), \"⁄©Ÿàÿ¶€å\"),\n",
    "    (re.compile(r\"\\b⁄©€Å\\b\"), \"⁄©€Å\"),  # noop example; placeholders for future\n",
    "]\n",
    "\n",
    "def _apply_variant_canon(s: str) -> str:\n",
    "    for pat, rep in _VARIANT_CANON:\n",
    "        s = pat.sub(rep, s)\n",
    "    return s\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Public normalizer\n",
    "# -----------------------------\n",
    "def normalize_urdu_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Robust normalizer for Urdu ASR scoring:\n",
    "    - Unicode compatibility & diacritics removal\n",
    "    - Canonical Urdu codepoints (Yeh/Heh goal)\n",
    "    - Remove Seamless-style '#um' disfluencies\n",
    "    - Remove punctuation\n",
    "    - Normalize digits (Latin/Arabic to Eastern Arabic-Indic)\n",
    "    - Canonicalize frequent orthographic variants (⁄Üÿß€Å€å€í, €ÅŸà⁄Øÿß~€ÅŸà ⁄Øÿß)\n",
    "    - Space squashing\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    s = text\n",
    "\n",
    "    # 1) Unicode & presentation forms ‚Üí canonical, drop tatweel/ZW & diacritics\n",
    "    s = _compat_normalize(s)\n",
    "\n",
    "    # 2) Canonical Urdu codepoints\n",
    "    s = _canonical_codepoints(s)\n",
    "\n",
    "    # 3) Disfluencies + punctuation\n",
    "    s = _strip_punct_and_disfluencies(s)\n",
    "\n",
    "    # 4) Digits (optional; or drop all digits if your refs omit numbers)\n",
    "    s = _normalize_digits(s)\n",
    "\n",
    "    # 5) Orthographic canonicalizations & token segmentation fixes\n",
    "    s = _apply_variant_canon(s)\n",
    "\n",
    "    # 6) Collapse spaces\n",
    "    s = _squash_spaces(s)\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Optional: \"lenient\" comparison for WER with variants\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Define lightweight variant generators for lattice expansion on very frequent cases.\n",
    "# Keep these sets tight to avoid combinatorial blow-up.\n",
    "_VARIANT_RULES = {\n",
    "    \"⁄Üÿß€Å€å€í\": {\"⁄Üÿß€Å€å€í\", \"⁄Üÿß€Åÿ¶€í\", \"⁄Üÿß€Å€åÿ¶€í\", \"⁄Üÿß⁄æ€å€í\", \"⁄Üÿß€Å€å€ì\"},\n",
    "    \"€ÅŸà⁄Øÿß\": {\"€ÅŸà⁄Øÿß\", \"€ÅŸà ⁄Øÿß\"},\n",
    "    \"€ÅŸà⁄Ø€å\": {\"€ÅŸà⁄Ø€å\", \"€ÅŸà ⁄Ø€å\"},\n",
    "    \"€ÅŸà⁄Ø€í\": {\"€ÅŸà⁄Ø€í\", \"€ÅŸà ⁄Ø€í\"},\n",
    "}\n",
    "\n",
    "def _expand_variants(tokens: List[str]) -> List[List[str]]:\n",
    "    expanded_per_token = []\n",
    "    for tok in tokens:\n",
    "        expanded_per_token.append(list(_VARIANT_RULES.get(tok, {tok})))\n",
    "    # Cartesian product over tokens to build candidate sequences\n",
    "    return [list(prod) for prod in product(*expanded_per_token)]\n",
    "\n",
    "def generate_lenient_variants(s: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Given a normalized string, produce a small set of alternative strings\n",
    "    accounting for the most common spelling/spacing variants.\n",
    "    \"\"\"\n",
    "    toks = s.split()\n",
    "    seqs = _expand_variants(toks)\n",
    "    return [\" \".join(seq) for seq in seqs]\n",
    "\n",
    "# Example of usage with jiwer:\n",
    "# from jiwer import wer as jiwer_wer\n",
    "# def lenient_min_wer(ref: str, hyp: str, normalizer: Callable[[str], str] = normalize_urdu_text) -> float:\n",
    "#     r = normalizer(ref)\n",
    "#     h = normalizer(hyp)\n",
    "#     r_cands = generate_lenient_variants(r)\n",
    "#     h_cands = generate_lenient_variants(h)\n",
    "#     # Compute min WER across small lattice of variants\n",
    "#     scores = []\n",
    "#     for rc in r_cands:\n",
    "#         for hc in h_cands:\n",
    "#             scores.append(jiwer_wer(rc, hc))\n",
    "#     return min(scores) if scores else jiwer_wer(r, h)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Text normalization function loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b8af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Experiment started: 2025-10-25 18:25:30\n",
      "‚úÖ Using device: cuda\n",
      "‚úÖ GPU: NVIDIA A40\n",
      "‚úÖ Available GPU memory: 48.31 GB\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SETUP\n",
    "# ================================\n",
    "\n",
    "overall_start_time = time.time()\n",
    "print(f\"üïê Experiment started: {datetime.datetime.fromtimestamp(overall_start_time).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Login to HuggingFace\n",
    "login(token=\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8d81c",
   "metadata": {},
   "source": [
    "# data laoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97b6beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# DATA LOADING HELPERS\n",
    "# ================================\n",
    "\n",
    "def ensure_audio_and_text(ds, text_keys=(\"transcription\", \"sentence\", \"text\", \"label\")):\n",
    "    \"\"\"Standardize column names to 'audio' and 'transcription'\"\"\"\n",
    "    # Ensure TEXT_COL\n",
    "    if TEXT_COL not in ds.column_names:\n",
    "        for k in text_keys:\n",
    "            if k in ds.column_names:\n",
    "                ds = ds.rename_column(k, TEXT_COL)\n",
    "                break\n",
    "    if TEXT_COL not in ds.column_names:\n",
    "        raise ValueError(\"Could not find transcript column\")\n",
    "\n",
    "    # Ensure AUDIO_COL and cast\n",
    "    if AUDIO_COL not in ds.column_names:\n",
    "        cand = next((c for c in ds.column_names if c.lower() in (\"audio\", \"path\", \"file\")), None)\n",
    "        if cand:\n",
    "            ds = ds.rename_column(cand, AUDIO_COL)\n",
    "    \n",
    "    # Always cast audio to ensure consistent sampling rate and format\n",
    "    ds = ds.cast_column(AUDIO_COL, Audio(sampling_rate=TARGET_SR, mono=True, decode=True))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def subsample_after_shuffle(ds, n, seed=RANDOM_SEED):\n",
    "    \"\"\"Shuffle and subsample dataset\"\"\"\n",
    "    if n is None or n <= 0 or n >= len(ds):\n",
    "        return ds\n",
    "    return ds.shuffle(seed=seed).select(range(n))\n",
    "\n",
    "def load_csalt_raw():\n",
    "    ds_all = load_dataset(\"urdu-asr/csalt-voice\", token=False)\n",
    "    train_like = ensure_audio_and_text(ds_all[\"validation\"])\n",
    "    return DatasetDict({\"train\": train_like})\n",
    "\n",
    "def load_fleurs_raw():\n",
    "    \"\"\"Load FLEURS Urdu (ur_pk + ur_in + ur) and merge all splits\"\"\"\n",
    "    all_langs = []\n",
    "    for lang_code in [\"ur_pk\", \"ur_in\", \"ur\"]:\n",
    "        try:\n",
    "            dataset = load_dataset(\"google/fleurs\", lang_code, trust_remote_code=True)\n",
    "            all_langs.append(dataset)\n",
    "            print(f\"‚úÖ Loaded FLEURS split for {lang_code} with splits: {list(dataset.keys())}\")\n",
    "        except Exception:\n",
    "            print(f\"‚ö†Ô∏è Could not load FLEURS language code: {lang_code}\")\n",
    "            continue\n",
    "\n",
    "    if not all_langs:\n",
    "        raise ValueError(\"Could not load any FLEURS Urdu variants\")\n",
    "\n",
    "    # Merge all language variants together\n",
    "    from datasets import DatasetDict, concatenate_datasets\n",
    "\n",
    "    merged = {}\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        merged_splits = [\n",
    "            ensure_audio_and_text(ds[split]) for ds in all_langs if split in ds\n",
    "        ]\n",
    "        if merged_splits:\n",
    "            merged[split] = concatenate_datasets(merged_splits)\n",
    "\n",
    "    print(f\"‚úÖ Combined FLEURS Urdu splits: {', '.join(merged.keys())}\")\n",
    "    return DatasetDict(merged)\n",
    "\n",
    "\n",
    "def load_commonvoice_raw():\n",
    "    ds_all = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"ur\", trust_remote_code=True, token=True)\n",
    "    dd = {}\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        if split in ds_all:\n",
    "            ds = ds_all[split]\n",
    "            if \"sentence\" in ds.column_names:\n",
    "                ds = ds.rename_column(\"sentence\", TEXT_COL)\n",
    "            dd[split] = ensure_audio_and_text(ds)\n",
    "    return DatasetDict(dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548cab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä LOADING DATASETS (dynamic)\n",
      "==================================================\n",
      "Loading CommonVoice...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\shaider\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\mozilla-foundation--common_voice_17_0\\9d10386a731ff6e6ed4ec973a4dc204a9820e8c842fbe388bdba0dd205ed5016 (last modified on Mon Oct 13 16:31:58 2025) since it couldn't be found locally at mozilla-foundation/common_voice_17_0, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FLEURS...\n",
      "‚úÖ Loaded FLEURS split for ur_pk with splits: ['train', 'validation', 'test']\n",
      "‚ö†Ô∏è Could not load FLEURS language code: ur_in\n",
      "‚ö†Ô∏è Could not load FLEURS language code: ur\n",
      "‚úÖ Combined FLEURS Urdu splits: train, validation, test\n",
      "Loading CSaLT...\n",
      "\n",
      "--------------------------------------------------\n",
      "üß© Building TRAIN pool from user choices...\n",
      "\n",
      "--------------------------------------------------\n",
      "üß™ Building TEST pool from user choices...\n",
      "\n",
      "==================================================\n",
      "‚úÖ FINAL DATASET SIZES\n",
      "==================================================\n",
      "Train set: 8000 samples\n",
      "Test set:  471 samples\n",
      "\n",
      "==================================================\n",
      "üìù DATASET SOURCES (for this run)\n",
      "==================================================\n",
      "train_1: commonvoice | train_2: fleurs | train_3: -\n",
      "test_1:  csalt  | test_2:  -  | test_3:  -\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# LOAD AND PREPARE DATASETS (DYNAMIC)\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä LOADING DATASETS (dynamic)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- USER CHOICES ------------------------------------------------------------\n",
    "# Pick from: \"commonvoice\", \"fleurs\", \"csalt\" or None\n",
    "# At minimum, train_1 and test_1 must be non-None\n",
    "train_1 = \"commonvoice\"\n",
    "train_2 = \"fleurs\"\n",
    "train_3 = None\n",
    "\n",
    "test_1  = \"csalt\"\n",
    "test_2  = None\n",
    "test_3  = None\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# 1) Load raw DatasetDicts (unchanged)\n",
    "print(\"Loading CommonVoice...\")\n",
    "commonvoice = load_commonvoice_raw()\n",
    "\n",
    "print(\"Loading FLEURS...\")\n",
    "fleurs = load_fleurs_raw()\n",
    "\n",
    "print(\"Loading CSaLT...\")\n",
    "csalt = load_csalt_raw()\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "def merge_all_splits(ds_dict):\n",
    "    \"\"\"\n",
    "    Concatenate all available splits from a DatasetDict.\n",
    "    This mirrors your previous logic (train+validation+test).\n",
    "    \"\"\"\n",
    "    available = [ds_dict[s] for s in [\"train\", \"validation\", \"test\"] if s in ds_dict]\n",
    "    if not available:\n",
    "        raise ValueError(\"No splits found to merge in provided DatasetDict.\")\n",
    "    return concatenate_datasets(available)\n",
    "\n",
    "def safe_select_columns(ds, wanted_cols):\n",
    "    \"\"\"\n",
    "    Select only the columns that actually exist to avoid KeyError\n",
    "    if a source is missing one. (Typically both AUDIO_COL and TEXT_COL exist.)\n",
    "    \"\"\"\n",
    "    keep = [c for c in wanted_cols if c in ds.column_names]\n",
    "    if not keep:\n",
    "        raise ValueError(\n",
    "            f\"None of the requested columns {wanted_cols} are present in {ds.column_names}\"\n",
    "        )\n",
    "    return ds.select_columns(keep)\n",
    "\n",
    "# 2) Build a prepared (merged + column-selected) registry for each dataset name\n",
    "prepared_registry = {\n",
    "    \"commonvoice\": safe_select_columns(merge_all_splits(commonvoice), [AUDIO_COL, TEXT_COL]),\n",
    "    \"fleurs\":      safe_select_columns(merge_all_splits(fleurs),      [AUDIO_COL, TEXT_COL]),\n",
    "    \"csalt\":       safe_select_columns(merge_all_splits(csalt),       [AUDIO_COL, TEXT_COL]),\n",
    "}\n",
    "\n",
    "# 3) Helpers to resolve user choices into a list of prepared datasets\n",
    "def resolve_choice(name: str | None):\n",
    "    if name is None:\n",
    "        return None\n",
    "    key = name.strip().lower()\n",
    "    if key not in prepared_registry:\n",
    "        valid = \", \".join(sorted(prepared_registry.keys()))\n",
    "        raise ValueError(f\"Unknown dataset '{name}'. Valid options: {valid} or None.\")\n",
    "    return prepared_registry[key]\n",
    "\n",
    "def build_pool(*names):\n",
    "    \"\"\"\n",
    "    Given up to three names/None, return a concatenated dataset\n",
    "    of all non-None selections. Requires at least one non-None.\n",
    "    \"\"\"\n",
    "    selected = [resolve_choice(n) for n in names if n is not None]\n",
    "    if not selected:\n",
    "        raise ValueError(\"At least one dataset must be selected to build a pool.\")\n",
    "    if len(selected) == 1:\n",
    "        return selected[0]\n",
    "    return concatenate_datasets(selected)\n",
    "\n",
    "# 4) Resolve TRAIN and TEST pools from the six choices\n",
    "#    (Shuffle + optional subsample mirrors your original behavior)\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"üß© Building TRAIN pool from user choices...\")\n",
    "train_pool = build_pool(train_1, train_2, train_3).shuffle(seed=RANDOM_SEED)\n",
    "\n",
    "# Optional subsampling (disabled if TRAIN_NUM_SAMPLES=None)\n",
    "train_ds = subsample_after_shuffle(train_pool, TRAIN_NUM_SAMPLES, seed=RANDOM_SEED)\n",
    "\n",
    "# Optional: carve validation from train (unchanged)\n",
    "validation_ds = None\n",
    "if EVAL_FROM_TRAIN_PCT > 0.0:\n",
    "    n_eval = int(len(train_ds) * EVAL_FROM_TRAIN_PCT)\n",
    "    if n_eval > 0:\n",
    "        validation_ds = train_ds.select(range(n_eval))\n",
    "        train_ds = train_ds.select(range(n_eval, len(train_ds)))\n",
    "        print(f\"‚úÖ Validation carved from train: {len(validation_ds)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"üß™ Building TEST pool from user choices...\")\n",
    "test_pool = build_pool(test_1, test_2, test_3).shuffle(seed=RANDOM_SEED)\n",
    "\n",
    "# Optional subsampling for test (same helper you already have)\n",
    "test_ds = subsample_after_shuffle(test_pool, TEST_NUM_SAMPLES, seed=RANDOM_SEED)\n",
    "\n",
    "# 5) Summaries\n",
    "def _fmt(x): return x if x is not None else \"-\"\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ FINAL DATASET SIZES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train set: {len(train_ds)} samples\")\n",
    "if validation_ds is not None:\n",
    "    print(f\"Validation set: {len(validation_ds)} samples\")\n",
    "print(f\"Test set:  {len(test_ds)} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìù DATASET SOURCES (for this run)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"train_1: {_fmt(train_1)} | train_2: {_fmt(train_2)} | train_3: {_fmt(train_3)}\")\n",
    "print(f\"test_1:  {_fmt(test_1)}  | test_2:  {_fmt(test_2)}  | test_3:  {_fmt(test_3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1122b33",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66318876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üîß MODEL SETUP\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded processor from openai/whisper-large-v3-turbo\n",
      "Loading model in FP16 precision...\n",
      "‚úÖ Configured model for Urdu transcription only (no English translation)\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 3,276,800 || all params: 812,154,880 || trainable%: 0.4035\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# MODEL SETUP\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîß MODEL SETUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load processor\n",
    "processor = WhisperProcessor.from_pretrained(BASE_MODEL_NAME)\n",
    "tokenizer = processor.tokenizer\n",
    "feature_extractor = processor.feature_extractor\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Loaded processor from {BASE_MODEL_NAME}\")\n",
    "\n",
    "# Load base model\n",
    "print(f\"Loading model in {'FP16' if FP16 else 'FP32'} precision...\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if FP16 else torch.float32\n",
    ")\n",
    "\n",
    "# ‚úÖ Force Urdu-only transcription mode (no English translation)\n",
    "model.config.forced_decoder_ids = None\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "model.config.language = \"ur\"\n",
    "model.config.task = \"transcribe\"\n",
    "model.generation_config.language = \"ur\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "print(\"‚úÖ Configured model for Urdu transcription only (no English translation)\")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    target_modules=LORA_TARGET_MODULES\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.forward = model.base_model.forward\n",
    "\n",
    "print(\"\\nüìä Trainable Parameters:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5722e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üîÑ PREPROCESSING DATA\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing train set: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [05:25<00:00, 24.55 examples/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# DATA PREPROCESSING\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîÑ PREPROCESSING DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    \"\"\"Preprocess audio and text for Whisper\"\"\"\n",
    "    audio = batch[AUDIO_COL]\n",
    "    \n",
    "    # Process audio\n",
    "    inputs = processor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "    \n",
    "    # Process text\n",
    "    tokenized = tokenizer(\n",
    "        batch[TEXT_COL],\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LABEL_LENGTH,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    batch[\"labels\"] = tokenized.input_ids[0]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Preprocess datasets\n",
    "train_ds = train_ds.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=train_ds.column_names,\n",
    "    desc=\"Preprocessing train set\"\n",
    ")\n",
    "\n",
    "if validation_ds:\n",
    "    validation_ds = validation_ds.map(\n",
    "        prepare_dataset,\n",
    "        remove_columns=validation_ds.column_names,\n",
    "        desc=\"Preprocessing validation set\"\n",
    "    )\n",
    "\n",
    "test_ds = test_ds.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=test_ds.column_names,\n",
    "    desc=\"Preprocessing test set\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üîç PRE-TRAINING WER EVALUATION\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-training evaluation:   0%|          | 0/471 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Pre-training evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 471/471 [10:25<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä PRE-TRAINING WER: 0.3111 (31.11%)\n",
      "\n",
      "üîç Sample Normalization Examples:\n",
      "\n",
      "Example 1:\n",
      "  Raw Reference:  ⁄©ÿ±€å⁄∫ ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ ÿßŸàÿ± €å€Å ÿ®ÿß€Åÿ± ⁄©ÿß ⁄©⁄æÿßŸÜÿß ÿ®⁄æ€å ⁄Ü⁄æŸà⁄ë ÿØ€å⁄∫ €ÅŸÖ ÿßÿµŸÑ ŸÖ€å⁄∫ €ÅŸÖ ŸÑŸà⁄Ø ÿ±€Åÿ™€í ÿ®⁄æ€å ÿ®ÿß€Åÿ± €Å€å⁄∫ ÿ™Ÿà Ÿæ⁄æÿ± €ÅŸÖÿßÿ±ÿß ÿßÿ≥ÿ™ÿπ\n",
      "  Norm Reference: ⁄©ÿ±€å⁄∫ ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ ÿßŸàÿ± €å€Å ÿ®ÿß€Åÿ± ⁄©ÿß ⁄©⁄æÿßŸÜÿß ÿ®⁄æ€å ⁄Ü⁄æŸà⁄ë ÿØ€å⁄∫ €ÅŸÖ ÿßÿµŸÑ ŸÖ€å⁄∫ €ÅŸÖ ŸÑŸà⁄Ø ÿ±€Åÿ™€í ÿ®⁄æ€å ÿ®ÿß€Åÿ± €Å€å⁄∫ ÿ™Ÿà Ÿæ⁄æÿ± €ÅŸÖÿßÿ±ÿß ÿßÿ≥ÿ™ÿπ\n",
      "  Raw Prediction: ⁄©ÿ±€å⁄∫ ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ ÿßŸàÿ± €å€Å ÿ®ÿß€Åÿ± ⁄©ÿß ÿÆÿßŸÜ€Å ÿ®⁄æ€å ⁄Ü⁄æŸà⁄ëÿ™€í €Å€å⁄∫ €ÅŸÖ ŸÖÿ´ŸÑ ŸÖ€å⁄∫ €ÅŸÖ ŸÑŸà⁄Ø ÿ±€Åÿ™€í €Å€å⁄∫ ÿ®⁄æ€å ÿ®ÿß€Åÿ± €Å€å⁄∫ ÿ™Ÿà Ÿæ⁄æÿ± €ÅŸÖÿßÿ±ÿß\n",
      "  Norm Prediction: ⁄©ÿ±€å⁄∫ ÿßÿ≥ÿ™ÿπŸÖÿßŸÑ ÿßŸàÿ± €å€Å ÿ®ÿß€Åÿ± ⁄©ÿß ÿÆÿßŸÜ€Å ÿ®⁄æ€å ⁄Ü⁄æŸà⁄ëÿ™€í €Å€å⁄∫ €ÅŸÖ ŸÖÿ´ŸÑ ŸÖ€å⁄∫ €ÅŸÖ ŸÑŸà⁄Ø ÿ±€Åÿ™€í €Å€å⁄∫ ÿ®⁄æ€å ÿ®ÿß€Åÿ± €Å€å⁄∫ ÿ™Ÿà Ÿæ⁄æÿ± €ÅŸÖÿßÿ±ÿß\n",
      "\n",
      "Example 2:\n",
      "  Raw Reference:  ÿßÿ≥ŸÑÿßŸÖ ÿπŸÑ€å⁄©ŸÖ ÿπŸÑ€åŸÜÿß\n",
      "  Norm Reference: ÿßÿ≥ŸÑÿßŸÖ ÿπŸÑ€å⁄©ŸÖ ÿπŸÑ€åŸÜÿß\n",
      "  Raw Prediction: ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑ€å⁄©ŸÖ ÿπŸÑ€åŸÜÿß\n",
      "  Norm Prediction: ÿßŸÑÿ≥ŸÑÿßŸÖ ÿπŸÑ€å⁄©ŸÖ ÿπŸÑ€åŸÜÿß\n",
      "\n",
      "Example 3:\n",
      "  Raw Reference:  ÿßŸà€Å €å€Å ÿ™Ÿà ÿ®€Åÿ™ ÿß⁄Ü⁄æÿß ⁄©€åÿß ÿ¢Ÿæ ŸÑŸà⁄ØŸà⁄∫ ŸÜ€í ÿß⁄Ü⁄æÿß\n",
      "  Norm Reference: ÿßŸà€Å €å€Å ÿ™Ÿà ÿ®€Åÿ™ ÿß⁄Ü⁄æÿß ⁄©€åÿß ÿ¢Ÿæ ŸÑŸà⁄ØŸà⁄∫ ŸÜ€í ÿß⁄Ü⁄æÿß\n",
      "  Raw Prediction: ÿßŸà€Å €å€Å ÿ™Ÿà ÿ®€Åÿ™ ÿß⁄Ü⁄æÿß ⁄©€åÿß ÿ¢Ÿæ ŸÑŸà⁄ØŸà⁄∫ ŸÜ€í ÿß⁄Ü⁄æÿß\n",
      "  Norm Prediction: ÿßŸà€Å €å€Å ÿ™Ÿà ÿ®€Åÿ™ ÿß⁄Ü⁄æÿß ⁄©€åÿß ÿ¢Ÿæ ŸÑŸà⁄ØŸà⁄∫ ŸÜ€í ÿß⁄Ü⁄æÿß\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# PRE-TRAINING EVALUATION\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üîç PRE-TRAINING WER EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def evaluate_model(model, test_dataset, device, desc=\"Evaluating\"):\n",
    "    \"\"\"Evaluate model and return WER metrics\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    predictions_raw = []  # Store raw predictions for debugging\n",
    "    references_raw = []   # Store raw references for debugging\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(test_dataset, desc=desc):\n",
    "            input_features = torch.tensor(sample[\"input_features\"]).unsqueeze(0).to(device)\n",
    "            \n",
    "            if FP16:\n",
    "                input_features = input_features.half()\n",
    "            \n",
    "            pred_ids = model.generate(input_features=input_features)\n",
    "            pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)[0].strip()\n",
    "            \n",
    "            # Get reference from the preprocessed labels\n",
    "            label_ids = sample[\"labels\"]\n",
    "            # Remove padding tokens\n",
    "            label_ids = [id for id in label_ids if id != tokenizer.pad_token_id]\n",
    "            label_str = tokenizer.decode(label_ids, skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Store raw versions\n",
    "            predictions_raw.append(pred_str)\n",
    "            references_raw.append(label_str)\n",
    "            \n",
    "            # *** APPLY TEXT NORMALIZATION HERE ***\n",
    "            pred_str_normalized = normalize_urdu_text(pred_str)\n",
    "            label_str_normalized = normalize_urdu_text(label_str)\n",
    "            \n",
    "            predictions.append(pred_str_normalized)\n",
    "            references.append(label_str_normalized)\n",
    "    \n",
    "    # Calculate WER on normalized text\n",
    "    sample_wers = [jiwer_wer(ref, pred) for ref, pred in zip(references, predictions)]\n",
    "    overall_wer = np.mean(sample_wers)\n",
    "    \n",
    "    return {\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references,\n",
    "        \"predictions_raw\": predictions_raw,  # Include raw for debugging\n",
    "        \"references_raw\": references_raw,\n",
    "        \"sample_wers\": sample_wers,\n",
    "        \"overall_wer\": overall_wer\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate before fine-tuning\n",
    "pre_results = evaluate_model(model, test_ds, device, desc=\"Pre-training evaluation\")\n",
    "pre_training_wer = pre_results[\"overall_wer\"]\n",
    "\n",
    "print(f\"\\nüìä PRE-TRAINING WER: {pre_training_wer:.4f} ({pre_training_wer*100:.2f}%)\")\n",
    "\n",
    "# Optional: Show some examples to verify normalization is working\n",
    "print(\"\\nüîç Sample Normalization Examples:\")\n",
    "for i in range(min(3, len(pre_results[\"predictions\"]))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Raw Reference:  {pre_results['references_raw'][i][:100]}\")\n",
    "    print(f\"  Norm Reference: {pre_results['references'][i][:100]}\")\n",
    "    print(f\"  Raw Prediction: {pre_results['predictions_raw'][i][:100]}\")\n",
    "    print(f\"  Norm Prediction: {pre_results['predictions'][i][:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8bb312c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèãÔ∏è TRAINING SETUP\n",
      "==================================================\n",
      "‚úÖ Optimizer: AdamW (lr=0.0001)\n",
      "‚úÖ Batch size: 8\n",
      "‚úÖ Total batches per epoch: 1000\n",
      "‚úÖ Mixed precision (FP16): True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaider\\AppData\\Local\\Temp\\ipykernel_24220\\2974741043.py:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if FP16 and torch.cuda.is_available() else None\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# TRAINING SETUP\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèãÔ∏è TRAINING SETUP\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for DataLoader\"\"\"\n",
    "    input_feats = torch.stack([\n",
    "        torch.tensor(item[\"input_features\"], dtype=torch.float32)\n",
    "        for item in batch\n",
    "    ])\n",
    "    \n",
    "    label_tensors = pad_sequence(\n",
    "        [torch.tensor(item[\"labels\"], dtype=torch.long) for item in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_features\": input_feats,\n",
    "        \"labels\": label_tensors\n",
    "    }\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Setup gradient scaler for FP16\n",
    "scaler = torch.cuda.amp.GradScaler() if FP16 and torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"‚úÖ Optimizer: AdamW (lr={LEARNING_RATE})\")\n",
    "print(f\"‚úÖ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"‚úÖ Total batches per epoch: {len(train_loader)}\")\n",
    "print(f\"‚úÖ Mixed precision (FP16): {FP16}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58936956",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633e0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üöÄ STARTING TRAINING\n",
      "==================================================\n",
      "\n",
      "üéØ Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1000 [00:00<?, ?it/s]C:\\Users\\shaider\\AppData\\Local\\Temp\\ipykernel_24220\\515463739.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [54:23<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1 complete ‚Äî Avg Loss: 0.1408\n",
      "\n",
      "üéØ Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [54:24<00:00,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2 complete ‚Äî Avg Loss: 0.0963\n",
      "\n",
      "‚úÖ Training complete! Duration: 1:48:47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# TRAINING\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "train_start_time = time.time()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    print(f\"\\nüéØ Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        try:\n",
    "            input_feats = batch[\"input_features\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Handle FP16 training\n",
    "            if FP16 and scaler is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(input_features=input_feats, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                if FP16:\n",
    "                    input_feats = input_feats.half()\n",
    "                \n",
    "                outputs = model(input_features=input_feats, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing batch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"‚úÖ Epoch {epoch+1} complete ‚Äî Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Validation if available\n",
    "    if validation_ds:\n",
    "        \n",
    "        val_results = evaluate_model(model, validation_ds, device, desc=\"Validation\")\n",
    "        print(f\"üîé Validation WER: {val_results['overall_wer']:.4f}\")\n",
    "        model.train()  # Back to training mode\n",
    "\n",
    "train_end_time = time.time()\n",
    "train_duration_secs = int(train_end_time - train_start_time)\n",
    "train_duration_hms = str(datetime.timedelta(seconds=train_duration_secs))\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Duration: {train_duration_hms}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949f2d6",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c5464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìè POST-TRAINING WER EVALUATION\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Post-training evaluation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 471/471 [10:17<00:00,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä POST-TRAINING WER: 0.2552 (25.52%)\n",
      "\n",
      "üéâ WER IMPROVEMENT: 0.0559 (17.96%)\n",
      "   Pre-training:  0.3111\n",
      "   Post-training: 0.2552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# POST-TRAINING EVALUATION\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìè POST-TRAINING WER EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "# Evaluate after fine-tuning\n",
    "post_results = evaluate_model(model, test_ds, device, desc=\"Post-training evaluation\")\n",
    "post_training_wer = post_results[\"overall_wer\"]\n",
    "\n",
    "print(f\"\\nüìä POST-TRAINING WER: {post_training_wer:.4f} ({post_training_wer*100:.2f}%)\")\n",
    "\n",
    "# Calculate improvement\n",
    "wer_improvement = pre_training_wer - post_training_wer\n",
    "wer_improvement_pct = (wer_improvement / pre_training_wer) * 100\n",
    "\n",
    "print(f\"\\nüéâ WER IMPROVEMENT: {wer_improvement:.4f} ({wer_improvement_pct:.2f}%)\")\n",
    "print(f\"   Pre-training:  {pre_training_wer:.4f}\")\n",
    "print(f\"   Post-training: {post_training_wer:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03044a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üíæ SAVING RESULTS\n",
      "==================================================\n",
      "üìÅ Created directory: ./finetuning-6\n",
      "üìÑ Saved predictions: ./finetuning-6/finetuning-6_predictions.csv\n",
      "üìÑ Saved summary: ./finetuning-6/finetuning-6_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SAVE RESULTS\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üíæ SAVING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "overall_end_time = time.time()\n",
    "overall_duration_secs = int(overall_end_time - overall_start_time)\n",
    "overall_duration_hms = str(datetime.timedelta(seconds=overall_duration_secs))\n",
    "\n",
    "# Save sample-wise predictions with both raw and normalized versions\n",
    "samplewise_data = []\n",
    "for i in range(len(post_results[\"predictions\"])):\n",
    "    samplewise_data.append({\n",
    "        \"reference_raw\": post_results[\"references_raw\"][i],\n",
    "        \"reference_normalized\": post_results[\"references\"][i],\n",
    "        \"prediction_raw\": post_results[\"predictions_raw\"][i],\n",
    "        \"prediction_normalized\": post_results[\"predictions\"][i],\n",
    "        \"wer\": round(post_results[\"sample_wers\"][i], 4)\n",
    "    })\n",
    "\n",
    "CSV_OUTPUT_DIR = f\"./{EXPERIMENT_NAME}\"\n",
    "# Create directories\n",
    "os.makedirs(CSV_OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Created directory: {CSV_OUTPUT_DIR}\")\n",
    "\n",
    "NEW_PREDICTIONS_CSV = f\"{CSV_OUTPUT_DIR}/{PREDICTIONS_CSV}\"\n",
    "pd.DataFrame(samplewise_data).to_csv(NEW_PREDICTIONS_CSV, index=False)\n",
    "print(f\"üìÑ Saved predictions: {NEW_PREDICTIONS_CSV}\")\n",
    "\n",
    "\n",
    "# Save run summary\n",
    "summary_data = {\n",
    "    \"experiment_name\": EXPERIMENT_NAME,\n",
    "    \"base_model\": BASE_MODEL_NAME,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"lora_dropout\": LORA_DROPOUT,\n",
    "    \"target_modules\": str(LORA_TARGET_MODULES),\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_train_epochs\": NUM_EPOCHS,\n",
    "    \"train_1\": train_1 if train_1 is not None else \"-\",\n",
    "    \"train_2\": train_2 if train_2 is not None else \"-\",\n",
    "    \"train_3\": train_3 if train_3 is not None else \"-\",\n",
    "    \"test_1\":  test_1 if test_1 is not None else \"-\",\n",
    "    \"test_2\":  test_2 if test_2 is not None else \"-\",\n",
    "    \"test_3\":  test_3 if test_3 is not None else \"-\",\n",
    "    \"train_num_samples_cap\": TRAIN_NUM_SAMPLES if TRAIN_NUM_SAMPLES else \"full\",\n",
    "    \"test_num_samples_cap\": TEST_NUM_SAMPLES if TEST_NUM_SAMPLES else \"full\",\n",
    "    \"eval_from_train_pct\": EVAL_FROM_TRAIN_PCT,\n",
    "    \"train_set_size\": len(train_ds),\n",
    "    \"validation_set_size\": len(validation_ds) if validation_ds else 0,\n",
    "    \"test_set_size\": len(test_ds),\n",
    "    \"total_start_time\": datetime.datetime.fromtimestamp(overall_start_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"total_end_time\": datetime.datetime.fromtimestamp(overall_end_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"total_duration\": overall_duration_hms,\n",
    "    \"train_start_time\": datetime.datetime.fromtimestamp(train_start_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"train_end_time\": datetime.datetime.fromtimestamp(train_end_time).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"train_duration\": train_duration_hms,\n",
    "    \"fp16_enabled\": FP16,\n",
    "    \"pre_training_wer\": round(pre_training_wer, 4),\n",
    "    \"post_training_wer\": round(post_training_wer, 4),\n",
    "    \"wer_improvement\": round(wer_improvement, 4),\n",
    "    \"wer_improvement_percent\": round(wer_improvement_pct, 2)\n",
    "}\n",
    "\n",
    "NEW_SUMMARY_CSV = f\"{CSV_OUTPUT_DIR}/{SUMMARY_CSV}\"\n",
    "pd.DataFrame([summary_data]).to_csv(NEW_SUMMARY_CSV, index=False)\n",
    "print(f\"üìÑ Saved summary: {NEW_SUMMARY_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üéâ EXPERIMENT COMPLETE\n",
      "==================================================\n",
      "Total duration: 2:28:04\n",
      "\n",
      "üìä Results:\n",
      "   Pre-training WER:  0.3111 (31.11%)\n",
      "   Post-training WER: 0.2552 (25.52%)\n",
      "   Improvement:       0.0559 (17.96%)\n",
      "\n",
      "üìÅ Output files:\n",
      "   - finetuning-6_predictions.csv\n",
      "   - finetuning-6_summary.csv\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# FINAL SUMMARY\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total duration: {overall_duration_hms}\")\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   Pre-training WER:  {pre_training_wer:.4f} ({pre_training_wer*100:.2f}%)\")\n",
    "print(f\"   Post-training WER: {post_training_wer:.4f} ({post_training_wer*100:.2f}%)\")\n",
    "print(f\"   Improvement:       {wer_improvement:.4f} ({wer_improvement_pct:.2f}%)\")\n",
    "print(f\"\\nüìÅ Output files:\")\n",
    "print(f\"   - {PREDICTIONS_CSV}\")\n",
    "print(f\"   - {SUMMARY_CSV}\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695044b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üíæ SAVING FINE-TUNED MODEL\n",
      "==================================================\n",
      "üìÅ Created directory: ./saved_models/finetuning-6/lora_adapter\n",
      "\n",
      "üîß Saving LoRA adapter weights...\n",
      "‚úÖ LoRA adapter saved to: ./saved_models/finetuning-6/lora_adapter\n",
      "‚úÖ Training config saved to: ./saved_models/finetuning-6/lora_adapter/training_config.json\n",
      "\n",
      "==================================================\n",
      "üéâ MODEL SAVING COMPLETE\n",
      "==================================================\n",
      "\n",
      "üì¶ Saved files:\n",
      "   LoRA Adapter: ./saved_models/finetuning-6/lora_adapter\n",
      "   - adapter_model.safetensors (LoRA weights)\n",
      "   - adapter_config.json (LoRA configuration)\n",
      "   - preprocessor_config.json & tokenizer files\n",
      "   - training_config.json (your training settings)\n",
      "\n",
      "üîÑ To load the model later, use:\n",
      "\n",
      "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
      "from peft import PeftModel\n",
      "\n",
      "# Load base model\n",
      "base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3-turbo\")\n",
      "\n",
      "# Load LoRA adapter\n",
      "model = PeftModel.from_pretrained(base_model, \"./saved_models/finetuning-6/lora_adapter\")\n",
      "\n",
      "# Load processor\n",
      "processor = WhisperProcessor.from_pretrained(\"./saved_models/finetuning-6/lora_adapter\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# SAVE FINE-TUNED MODEL\n",
    "# ================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üíæ SAVING FINE-TUNED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Define output directory\n",
    "OUTPUT_DIR = f\"./saved_models/{EXPERIMENT_NAME}\"\n",
    "LORA_ADAPTER_DIR = f\"{OUTPUT_DIR}/lora_adapter\"\n",
    "MERGED_MODEL_DIR = f\"{OUTPUT_DIR}/merged_model\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(LORA_ADAPTER_DIR, exist_ok=True)\n",
    "print(f\"üìÅ Created directory: {LORA_ADAPTER_DIR}\")\n",
    "\n",
    "# 1. Save LoRA adapter weights (lightweight, recommended)\n",
    "print(\"\\nüîß Saving LoRA adapter weights...\")\n",
    "model.save_pretrained(LORA_ADAPTER_DIR)\n",
    "processor.save_pretrained(LORA_ADAPTER_DIR)\n",
    "print(f\"‚úÖ LoRA adapter saved to: {LORA_ADAPTER_DIR}\")\n",
    "\n",
    "# 2. Save the merged model (optional, larger file size)\n",
    "# Uncomment the following lines if you want to save the full merged model\n",
    "\"\"\"\n",
    "print(\"\\nüîÑ Merging LoRA weights with base model...\")\n",
    "os.makedirs(MERGED_MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Merge and unload LoRA weights\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "print(\"üíæ Saving merged model...\")\n",
    "merged_model.save_pretrained(MERGED_MODEL_DIR)\n",
    "processor.save_pretrained(MERGED_MODEL_DIR)\n",
    "print(f\"‚úÖ Merged model saved to: {MERGED_MODEL_DIR}\")\n",
    "\"\"\"\n",
    "\n",
    "# 3. Save configuration info\n",
    "config_info = {\n",
    "    \"base_model\": BASE_MODEL_NAME,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"lora_dropout\": LORA_DROPOUT,\n",
    "    \"target_modules\": LORA_TARGET_MODULES,\n",
    "    \"training_epochs\": NUM_EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"final_wer\": round(post_training_wer, 4)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f\"{LORA_ADAPTER_DIR}/training_config.json\", \"w\") as f:\n",
    "    json.dump(config_info, f, indent=2)\n",
    "print(f\"‚úÖ Training config saved to: {LORA_ADAPTER_DIR}/training_config.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ MODEL SAVING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nüì¶ Saved files:\")\n",
    "print(f\"   LoRA Adapter: {LORA_ADAPTER_DIR}\")\n",
    "print(f\"   - adapter_model.safetensors (LoRA weights)\")\n",
    "print(f\"   - adapter_config.json (LoRA configuration)\")\n",
    "print(f\"   - preprocessor_config.json & tokenizer files\")\n",
    "print(f\"   - training_config.json (your training settings)\")\n",
    "\n",
    "print(f\"\\nüîÑ To load the model later, use:\")\n",
    "print(f\"\"\"\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\"{BASE_MODEL_NAME}\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"{LORA_ADAPTER_DIR}\")\n",
    "\n",
    "# Load processor\n",
    "processor = WhisperProcessor.from_pretrained(\"{LORA_ADAPTER_DIR}\")\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
